{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56be47c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799e5690",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3304316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "///////////enter sdl.py//////////\n",
      "UE_KEY_LIST= {'UEPDCPBytesDL', 'UEPDCPBytesUL', 'MeasPeriodUEPRBUsage', 'UEPRBUsageDL', 'MeasPeriodUEPDCPBytes', 'MeasTimestampUEPDCPBytes', 'UEPRBUsageUL', 'ServingCellID', 'MeasTimestampUEPRBUsage'}\n",
      "CELL_KEY_LIST= {'MeasPeriodPDCPBytes', 'PDCPBytesUL', 'MeasTimestampAvailPRB', 'CellID', 'PDCPBytesDL', 'MeasPeriodAvailPRB', 'AvailPRBUL', 'MeasTimestampPDCPBytes', 'AvailPRBDL'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohammadreza\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\Mohammadreza\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "C:\\Users\\Mohammadreza\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "///////////enter class DBCreatDrop/////////////\n",
      "///////////enter class DATABASE(object)////////////////////\n",
      "////////enter class Error in db////////////////\n",
      "////////enter class NoDataError in db//////////////////\n",
      "///////////////enter INSERTDATA class in populate/////////////\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==================================================================================\n",
    "#       Copyright (c) 2020 China Mobile Technology (USA) Inc. Intellectual Property.\n",
    "#\n",
    "#   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#   you may not use this file except in compliance with the License.\n",
    "#   You may obtain a copy of the License at\n",
    "#\n",
    "#          http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#   Unless required by applicable law or agreed to in writing, software\n",
    "#   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#   See the License for the specific language governing permissions and\n",
    "#   limitations under the License.\n",
    "# ==================================================================================\n",
    "\"\"\"\n",
    "lp entrypoint module\n",
    "\n",
    "RMR Messages\n",
    " #define TS_UE_LIST 30000\n",
    "for now re-use the 30000 to receive a UEID for prediction\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('C:/Users/Mohammadreza/Desktop/My Class/Proj-DC/My Works/Scheduling/xApp/mr7-main')\n",
    "import schedule\n",
    "from zipfile import ZipFile\n",
    "import json\n",
    "from os import getenv\n",
    "#from ricxappframe.xapp_frame import RMRXapp, rmr, Xapp\n",
    "from mr import sdl\n",
    "#from lp.exceptions import UENotFound, CellNotFound\n",
    "\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from numpy import zeros, newaxis\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from mr.db import DATABASE, DUMMY\n",
    "import mr.populate as populate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f84ad103",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential  # To compose multiple Layers\n",
    "from tensorflow.keras.layers import LSTM, Dense  # Fully-Connected layer\n",
    "from tensorflow.keras.layers import Activation  # Activation functions\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b4b3ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 9\n",
    "\n",
    "def LSTMClassifier():\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(9, return_sequences=True, activation='relu', recurrent_activation='sigmoid', input_shape=(9,1)))\n",
    "\n",
    "    model.add(LSTM(16, return_sequences=True, activation='relu', recurrent_activation='sigmoid'))\n",
    "\n",
    "    model.add(LSTM(32, return_sequences=False, activation='relu', recurrent_activation='sigmoid'))\n",
    "\n",
    "    model.add(Dense(32))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # model.add(Dense(num_actions))\n",
    "    # model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "    # model.add(Dense(num_actions))\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('linear'))\n",
    "\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=0.0001))\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cb22038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first line of code: xapp=None= None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "xapp = None\n",
    "print('first line of code: xapp=None=', xapp)\n",
    "pos = 0\n",
    "cell_data = None\n",
    "rmr_xapp = None\n",
    "ai_model = None\n",
    "\n",
    "class UENotFound(BaseException):\n",
    "    pass\n",
    "class CellNotFound(BaseException):\n",
    "    pass\n",
    "\n",
    "def post_init(self):\n",
    "    print('///////enter def post_init__/////////////////')\n",
    "    \"\"\"\n",
    "    Function that runs when xapp initialization is complete\n",
    "    \"\"\"\n",
    "    self.def_hand_called = 0\n",
    "    self.traffic_steering_requests = 0\n",
    "\n",
    "\n",
    "def handle_config_change(self, config):\n",
    "    print('////////enter def handle_config_change//////////////')\n",
    "    \"\"\"\n",
    "    Function that runs at start and on every configuration file change.\n",
    "    \"\"\"\n",
    "    self.logger.debug(\"handle_config_change: config: {}\".format(config))\n",
    "\n",
    "\n",
    "def default_handler(self, summary, sbuf):\n",
    "    print('/////////enter def default_handler///////////////')\n",
    "    \"\"\"\n",
    "    Function that processes messages for which no handler is defined\n",
    "    \"\"\"\n",
    "    self.def_hand_called += 1\n",
    "    print('self.def_hand_called += 1=', self.def_hand_called)\n",
    "    self.logger.warning(\"default_handler unexpected message type {}\".format(summary[rmr.RMR_MS_MSG_TYPE]))\n",
    "    self.rmr_free(sbuf)\n",
    "\n",
    "\n",
    "def mr_req_handler(self, summary, sbuf):\n",
    "    print('///////////enter def mr_req handler/////////////')\n",
    "    \"\"\"\n",
    "    This is the main handler for this xapp, which handles load prediction requests.\n",
    "    This app fetches a set of data from SDL, and calls the predict method to perform\n",
    "    prediction based on the data\n",
    "\n",
    "    The incoming message that this function handles looks like:\n",
    "        {\"UEPredictionSet\" : [\"UEId1\",\"UEId2\",\"UEId3\"]}\n",
    "    \"\"\"\n",
    "    self.traffic_steering_requests += 1\n",
    "    # we don't use rts here; free the buffer\n",
    "    self.rmr_free(sbuf)\n",
    "\n",
    "    ue_list = []\n",
    "    try:\n",
    "        print('////enter first try in mr_req_handler////')\n",
    "        print('rmr.RMR_MS_PAYLOAD=', rmr.RMR_MS_PAYLOAD)\n",
    "        print('summary[rmr.RMR_MS_PAYLOAD]=', summary[rmr.RMR_MS_PAYLOAD])\n",
    "        req = json.loads(summary[rmr.RMR_MS_PAYLOAD])  # input should be a json encoded as bytes\n",
    "        print('req = json.loads(summary[rmr.RMR_MS_PAYLOAD])=', req)\n",
    "        ue_list = req[\"UEPredictionSet\"]\n",
    "        print('ue_list=req[\"UEPredictionSet\"] =', ue_list)\n",
    "        self.logger.debug(\"mr_req_handler processing request for UE list {}\".format(ue_list))\n",
    "    except (json.decoder.JSONDecodeError, KeyError):\n",
    "        print('////enter first except in mr_req_handler////')\n",
    "        self.logger.warning(\"mr_req_handler failed to parse request: {}\".format(summary[rmr.RMR_MS_PAYLOAD]))\n",
    "        return\n",
    "    print('ue_list mr_req_handler aftr 1st try=', ue_list)\n",
    "    # iterate over the UEs, fetches data for each UE and perform prediction\n",
    "    for ueid in ue_list:\n",
    "        try:\n",
    "            print('////enter second try in mr_req_handler////')\n",
    "            uedata = sdl.get_uedata(self, ueid)\n",
    "            print('uedata = sdl.get_uedata(self, ueid)=', uedata)\n",
    "            predict(self, uedata)\n",
    "            print('predict(self, uedata)=', predict(self, uedata))\n",
    "        except UENotFound:\n",
    "            print('////enter second except in mr_req_handler////')\n",
    "            print('enter UENotFound in mr_req_handler')\n",
    "            self.logger.warning(\"mr_req_handler received a TS Request for a UE that does not exist!\")\n",
    "\n",
    "def entry():\n",
    "    print('////////////enter def entry///////////////')\n",
    "    \"\"\"  Read from DB in an infinite loop and run prediction every second\n",
    "      TODO: do training as needed in the future\n",
    "    \"\"\"\n",
    "    schedule.every(1).seconds.do(run_prediction)\n",
    "    print('/////////pass 1 entry schedule.every(1).seconds.do(run_prediction, self)/////')\n",
    "    while True:\n",
    "        #print('////while True in entry/////') \n",
    "        schedule.run_pending()\n",
    "\n",
    "def run_prediction():\n",
    "    print('///////////////enter def run_prediction///////////////')\n",
    "    \"\"\"Read the latest cell_meas sample from influxDB and run it by the model inference\n",
    "    \"\"\"\n",
    "\n",
    "    global pos\n",
    "    sample = [3735, 0, 27648, 2295, 18, -1, 16383,-1, -1, -1]\n",
    "    print('sample=[3735,...]=', sample)\n",
    "    if cell_data:\n",
    "        print('//////////enter if cell_data in run_prediction/////')\n",
    "        print('cell_data=', cell_data)\n",
    "        pos = (pos + 1) % len(cell_data)  # iterate through entire list one at a time\n",
    "        print('pos=(pos + 1) % len(cell_data)=', pos)\n",
    "        sample = cell_data[pos]\n",
    "        print('sample = cell_data[pos]=', sample)\n",
    "    predict(sample)\n",
    "\n",
    "def predict(celldata):\n",
    "    print('/////////////enter def predict/////////////////')      \n",
    "    \"\"\"\n",
    "    This is the method that's to perform prediction based on a model\n",
    "    For now it just returns dummy data\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    ai_model = load_model_parameter()\n",
    "    print('ai_model.summary()=', ai_model)\n",
    "    print('celldata:', celldata)\n",
    "    ret = predict_unseen_data(ai_model, celldata)\n",
    "    print('ret=predict_unseen_data(ai_model, celldata)=', ret)\n",
    "    print('celldata:', celldata)\n",
    "    print('Classification', ret)\n",
    "    print('////prediction doneee///////')\n",
    "    return ret\n",
    "\n",
    "def load_model_parameter():\n",
    "    print('/////////////enter def load_model_parameters////////////////')\n",
    "    PATH = 'model.pth'\n",
    "    print('PATH = model.pth=', PATH)      \n",
    "    cwd = os.getcwd()\n",
    "    print('cwd=os.getcwd=', cwd)\n",
    "    print('os.listdir(cwd)=', os.listdir(cwd))\n",
    "    if not os.path.exists(PATH):\n",
    "        print('///enter if not os.path.exists(PATH):////')\n",
    "        with ZipFile('C:/Users/Mohammadreza/Desktop/My Class/Proj-DC/My Works/Scheduling/xApp/mr7-main/mr/model.zip', 'r') as zip:\n",
    "            zip.printdir()\n",
    "            zip.extractall()\n",
    "    #input_dim = 10\n",
    "    input_dim = 9\n",
    "    hidden_dim = 256\n",
    "    layer_dim = 3\n",
    "    output_dim = 2\n",
    "    device = \"cpu\"\n",
    "    #model = LSTMClassifier(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "    #print('model=LSTClassifier=', model)\n",
    "    \n",
    "    model = LSTMClassifier()\n",
    "    print('model=LSTClassifier=', model.summary())\n",
    "    \n",
    "    model.save(cwd)\n",
    "    \n",
    "    model = tf.keras.models.load_model(cwd)\n",
    "    \n",
    "#     print(\"Model's state_dict:\")\n",
    "#     for param_tensor in model.state_dict():\n",
    "#         print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "#     # model = model.to(device)\n",
    "#     torch.save(model.state_dict(), PATH)\n",
    "    \n",
    "#     model.load_state_dict(torch.load(PATH))\n",
    "#     model.eval()\n",
    "#     print('model.eval()=', model)      \n",
    "    # print(model)\n",
    "    return model\n",
    "\n",
    "def predict_unseen_data(model, unseen_data):\n",
    "    print('/////////////////enter def predict_unseen_data///////////////////')\n",
    "    print('unseen_data=', unseen_data)\n",
    "    #print('unseen_data[measTimeStampRf]=', unseen_data['measTimeStampRf'])\n",
    "    np_data = np.asarray(unseen_data, dtype=np.float32)\n",
    "    print('np_data=', np_data)\n",
    "    print('newaxis=', newaxis)\n",
    "    X_grouped = np_data[newaxis, newaxis, :]\n",
    "    print('X_grouped = np_data[newaxis, newaxis, :]=',  X_grouped)\n",
    "    print('X_grouped.transpose(0, 2, 1)=', X_grouped.transpose(0, 2, 1))\n",
    "    \n",
    "#     print('torch.tensor(X_grouped.transpose(0, 2, 1))=', torch.tensor(X_grouped.transpose(0, 2, 1)))\n",
    "#     X_grouped = torch.tensor(X_grouped.transpose(0, 2, 1)).float()\n",
    "#     print('X_grouped=', X_grouped)\n",
    "#     y_fake = torch.tensor([0] * len(X_grouped)).long()\n",
    "#     print('y_fake=', y_fake)\n",
    "    \n",
    "    print('X_grouped.transpose=', X_grouped.transpose)\n",
    "    X_grouped = X_grouped.transpose(0,2,1)\n",
    "    print('X_grouped.transpose(0,2,1)=', X_grouped)\n",
    "    X_grouped = tf.convert_to_tensor(X_grouped, dtype=tf.float32)\n",
    "    print('X_grouped=', X_grouped)\n",
    "    y_fake = tf.convert_to_tensor([0] * len(X_grouped), dtype=tf.int64)\n",
    "    print('y_fake=', y_fake)\n",
    "    \n",
    "#     tensor_test = TensorDataset(X_grouped, y_fake)\n",
    "#     print('tensor_test= TensorDataset(X_grouped, y_fake)=', tensor_test)\n",
    "    \n",
    "    tensor_test = tf.data.Dataset.from_tensors((X_grouped, y_fake))\n",
    "    print('tensor_test= TensorDataset(X_grouped, y_fake)=', tensor_test)\n",
    "    \n",
    "#     test_dl = DataLoader(tensor_test, batch_size=1, shuffle=False)\n",
    "#     print('test_dl=', test_dl)    \n",
    "    \n",
    "    test_dl = tensor_test.batch(1)\n",
    "    print('test_dl=', test_dl) \n",
    "    \n",
    "    model.fit(tensor_test, epochs=20, verbose=1)\n",
    "    \n",
    "    ret = []\n",
    "    for batch, _ in test_dl:\n",
    "        print('///enter for loop in predict unseen data////')\n",
    "        #batch = batch.permute(0, 2, 1)\n",
    "        print('batch=', batch)\n",
    "        batch = batch[0]\n",
    "        print('batch = batch[0]=', batch)\n",
    "        #batch = tf.transpose(batch, perm=[0,2,1])\n",
    "        #batch = tf.transpose(batch)\n",
    "        #print('batch = tf.transpose(batch, (0,2,1))=', batch)\n",
    "        \n",
    "        #batch = batch.transpose(0,2,1)\n",
    "        #print('batch = batch.transpose(0,2,1)=', batch)\n",
    "        #batch = tf.keras.layers.Permute((0,2,1), batch)\n",
    "        #print('batch= batch.permute(0, 2, 1)=', batch)\n",
    "        \n",
    "        out = model.predict(batch)\n",
    "        print('out=model(batch)=', out)\n",
    "        \n",
    "        y_hat = tf.nn.log_softmax(out)\n",
    "        print('y_hat = tf.nn.log_softmax(out)=', y_hat)\n",
    "        y_hat = tf.argmax(y_hat)\n",
    "        print('y_hat = tf.argmax(y_hat)=', y_hat)\n",
    "        #y_hat = F.log_softmax(out, dim=1).argmax(dim=1)\n",
    "        #print('y_hat= F.log_softmax(out, dim=1).argmax(dim=1)=', y_hat)\n",
    "        \n",
    "        y_hat = tf.get_static_value(y_hat, partial=False)\n",
    "        print('y_hat = tf.get_static_value(y_hat, partial=False)=', y_hat)\n",
    "        \n",
    "        ret += y_hat.tolist()\n",
    "        print('ret += y_hat.tolist()=', ret)\n",
    "    print('ret after for loop=', ret)\n",
    "    print('ret[0]', ret[0]) \n",
    "    if ret[0] == 0:\n",
    "        print('ret[0] == 0')  \n",
    "        return \"Normal\"\n",
    "    return \"Congestion\"\n",
    "\n",
    "def connectdb(thread=False):\n",
    "    print('////////////////////enter def connectdb///////////////////')\n",
    "    # Create a connection to InfluxDB if thread=True, otherwise it will create a dummy data instance\n",
    "    global db\n",
    "    global cell_data\n",
    "    if thread:\n",
    "        print('///////////////enter if(thread) in connectdb///////////////////')  \n",
    "        db = DUMMY()\n",
    "        print('db =DUMMY()=', db)  \n",
    "    else:\n",
    "        print('//////enter else= populate.populate()////////////////')  \n",
    "        populate.populatedb()  # temporary method to populate db, it will be removed when data will be coming through KPIMON to influxDB\n",
    "        \n",
    "        print('////came back from populate to connectdb.else:, db=DATABASE(CellData)///////')\n",
    "        db = DATABASE('CellData')\n",
    "        print('////came back from db.DATABASE-init to connectdb.else///////')\n",
    "        print('db =  DATABASE(celldata) =', db) \n",
    "        db.read_data(\"cellMeas\")\n",
    "        print('////came back from db.DATABASE-read-data to connectdb.else///////')\n",
    "        print('db.read_data(\"cellMeas\")=', db.read_data(\"cellMeas\"))\n",
    "        cell_data = db.data.values.tolist()  # needs to be updated in future when live feed will be coming through KPIMON to influxDB\n",
    "        print('cell_data = db.data.values.tolist()=', cell_data)\n",
    "        #print('cell_data:, cell_data)\n",
    "        print('///////connectdb finished go to start//////')\n",
    "\n",
    "def start(thread=False):\n",
    " \n",
    "    print('////////////////entered Starrrrrrrrrrrt///////////////////')\n",
    "    \"\"\"\n",
    "    This is a convenience function that allows this xapp to run in Docker\n",
    "    for \"real\" (no thread, real SDL), but also easily modified for unit testing\n",
    "    (e.g., use_fake_sdl). The defaults for this function are for the Dockerized xapp.\n",
    "    \"\"\"\n",
    "    global xapp, ai_model\n",
    "    #fake_sdl = getenv(\"USE_FAKE_SDL\", None)\n",
    "    #xapp = Xapp(entrypoint=entry, rmr_port=4560, use_fake_sdl=False)\n",
    "    \n",
    "    connectdb(thread)\n",
    "    print('///////come back from connectdb////////')\n",
    "    ai_model = load_model_parameter()\n",
    "    print('ai_model.summary in start=', ai_model)\n",
    "    \n",
    "    use_fake_sdl=False\n",
    "    rmr_port=4560\n",
    "    #xapp= entry(self)\n",
    "    #xapp.run()\n",
    "    entry()\n",
    "    \n",
    "    #fake_sdl = getenv(\"USE_FAKE_SDL\", None)\n",
    "    #xapp = Xapp(entrypoint=entry, rmr_port=4560, use_fake_sdl=False)\n",
    "    #print('xapp = Xapp(entrypoint=entry, rmr_port=4560, use_fake_sdl=fake_sdl)=', xapp)\n",
    "    #connectdb(thread)\n",
    "    #ai_model = load_model_parameter()\n",
    "    #print('ai_model.summary=', ai_model)\n",
    "    \n",
    "    #xapp.run()\n",
    "\n",
    "\n",
    "def stop():\n",
    "    print('/////////////enter def stop//////////////////')      \n",
    "    \"\"\"\n",
    "    can only be called if thread=True when started\n",
    "    \"\"\"\n",
    "    xapp.stop()\n",
    "\n",
    "\n",
    "def get_stats():\n",
    "    print('//////////////////enter def get_stats()////////////////////')\n",
    "    \"\"\"\n",
    "    hacky for now, will evolve\n",
    "    \"\"\"\n",
    "    print('DefCalled:rmr_xapp.def_hand_called=', rmr_xapp.def_hand_called)\n",
    "    print('SteeringRequests:rmr_xapp.traffic_steering_requests=', rmr_xapp.traffic_steering_requests) \n",
    "    return {\"DefCalled\": rmr_xapp.def_hand_called,\n",
    "            \"SteeringRequests\": rmr_xapp.traffic_steering_requests}\n",
    "\n",
    "# class LSTMClassifier(nn.Module):\n",
    "#     print('/////////////////enter class LSTClassifier////////////////////')      \n",
    "#     def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "#         print('///////enter def __init__ in LST//////////////////')  \n",
    "#         super().__init__()\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         print('self.hidden_dim=', self.hidden_dim)\n",
    "#         self.layer_dim = layer_dim\n",
    "#         print('self.layer_dim=', self.layer_dim)\n",
    "#         self.rnn = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "#         self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "#         self.batch_size = None\n",
    "#         self.hidden = None\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         print('////////enter def forward in LST//////////////////') \n",
    "#         h0, c0 = self.init_hidden(x)\n",
    "#         print('h0, c0 = self.init_hidden(x)', h0)\n",
    "#         print('h0, c0 = self.init_hidden(x)', c0)\n",
    "#         out, (hn, cn) = self.rnn(x, (h0, c0))\n",
    "#         out = self.fc(out[:, -1, :])\n",
    "#         print('out = self.fc(out[:, -1, :])=', out)\n",
    "#         return out\n",
    "\n",
    "#     def init_hidden(self, x):\n",
    "#         print('/////////enter def init_hidden in LST//////////////////')   \n",
    "#         h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim)\n",
    "#         print('h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim)', h0)\n",
    "#         c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim)\n",
    "#         print('c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim)', c0)\n",
    "#         return [t for t in (h0, c0)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7807f824",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0d0472a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "////////////////entered Starrrrrrrrrrrt///////////////////\n",
      "////////////////////enter def connectdb///////////////////\n",
      "//////enter else= populate.populate()////////////////\n",
      "/////////////enter def populatedb()///////////\n",
      "data = pd.read_csv(mr/cells.csv)=    du-id          measTimeStampRf  availPrbDl  availPrbUl  measPeriodPrb  \\\n",
      "0   1001  2020-09-16T13:57:48.220         273         273             10   \n",
      "\n",
      "   nrCellIdentity  pdcpBytesUl  pdcpBytesDl  measPeriodPdcpBytes  \n",
      "0               2      5000000      5000000                   10  \n",
      "///////////////enter def time//////////////\n",
      "df.index= DatetimeIndex(['2022-08-13 18:33:59.684690'], dtype='datetime64[ns]', freq='10L')\n",
      "2022-08-13 18:33:59.684690    2020-09-16T13:57:48.220\n",
      "Freq: 10L, Name: measTimeStampRf, dtype: object\n",
      "lambda x: str(x)= <function time.<locals>.<lambda> at 0x000001EAF678C280>\n",
      "df=                             du-id          measTimeStampRf  availPrbDl  \\\n",
      "2022-08-13 18:33:59.684690   1001  2020-09-16T13:57:48.220         273   \n",
      "\n",
      "                            availPrbUl  measPeriodPrb  nrCellIdentity  \\\n",
      "2022-08-13 18:33:59.684690         273             10               2   \n",
      "\n",
      "                            pdcpBytesUl  pdcpBytesDl  measPeriodPdcpBytes  \n",
      "2022-08-13 18:33:59.684690      5000000      5000000                   10  \n",
      "data after time(data)=                             du-id          measTimeStampRf  availPrbDl  \\\n",
      "2022-08-13 18:33:59.684690   1001  2020-09-16T13:57:48.220         273   \n",
      "\n",
      "                            availPrbUl  measPeriodPrb  nrCellIdentity  \\\n",
      "2022-08-13 18:33:59.684690         273             10               2   \n",
      "\n",
      "                            pdcpBytesUl  pdcpBytesDl  measPeriodPdcpBytes  \n",
      "2022-08-13 18:33:59.684690      5000000      5000000                   10  \n",
      "///////enter insert init////////\n",
      "host= localhost\n",
      "self.client= <influxdb._dataframe_client.DataFrameClient object at 0x000001EAF6706A30>\n",
      "//////////enter insert dropdb/////////\n",
      "DROP database: CellData\n",
      "/////pass dropdb(CellData)////////\n",
      "///////enter insert createdb//////////\n",
      "Create database: CellData\n",
      "self.client.get_list_database()= [{'name': '_internal'}, {'name': 'database_01.csv'}, {'name': 'database_05'}, {'name': 'antar2'}, {'name': 'mydb_01'}, {'name': 'CellData'}]\n",
      "/////pass creatdb(CellData)////////\n",
      "insert data finished, go to write_point\n",
      "db = <mr.populate.INSERTDATA object at 0x000001EAF6706340>\n",
      "db.client.write_points(data, cellMeas)= True\n",
      "////came back from populate to connectdb.else:, db=DATABASE(CellData)///////\n",
      "///////enter def __init__ in class DATABASE////////////////\n",
      "self.client= <influxdb._dataframe_client.DataFrameClient object at 0x000001EAF67917F0>\n",
      "////came back from db.DATABASE-init to connectdb.else///////\n",
      "db =  DATABASE(celldata) = <mr.db.DATABASE object at 0x000001EAF6791700>\n",
      "///////enter def read_data(self, meas, limit=100): in class DATABASE///////////\n",
      "meas= cellMeas\n",
      "limit= 100\n",
      "str(limit)= 100\n",
      "self.client.get_list_database()= [{'name': '_internal'}, {'name': 'database_01.csv'}, {'name': 'database_05'}, {'name': 'antar2'}, {'name': 'mydb_01'}, {'name': 'CellData'}]\n",
      "self.client.get_list_measurements()= [{'name': 'cellMeas'}]\n",
      "result= defaultdict(<class 'list'>, {'cellMeas':                                   availPrbDl  availPrbUl  du-id  \\\n",
      "2022-08-13 18:33:59.684690+00:00         273         273   1001   \n",
      "\n",
      "                                  measPeriodPdcpBytes  measPeriodPrb  \\\n",
      "2022-08-13 18:33:59.684690+00:00                   10             10   \n",
      "\n",
      "                                          measTimeStampRf  nrCellIdentity  \\\n",
      "2022-08-13 18:33:59.684690+00:00  2020-09-16T13:57:48.220               2   \n",
      "\n",
      "                                  pdcpBytesDl  pdcpBytesUl  \n",
      "2022-08-13 18:33:59.684690+00:00      5000000      5000000  })\n",
      "Querying data : cellMeas : size - 1\n",
      "/////enter try in read-data def///\n",
      "result[meas]=                                   availPrbDl  availPrbUl  du-id  \\\n",
      "2022-08-13 18:33:59.684690+00:00         273         273   1001   \n",
      "\n",
      "                                  measPeriodPdcpBytes  measPeriodPrb  \\\n",
      "2022-08-13 18:33:59.684690+00:00                   10             10   \n",
      "\n",
      "                                          measTimeStampRf  nrCellIdentity  \\\n",
      "2022-08-13 18:33:59.684690+00:00  2020-09-16T13:57:48.220               2   \n",
      "\n",
      "                                  pdcpBytesDl  pdcpBytesUl  \n",
      "2022-08-13 18:33:59.684690+00:00      5000000      5000000  \n",
      "len(result[meas])= 1\n",
      "self.data==                                   availPrbDl  availPrbUl  du-id  \\\n",
      "2022-08-13 18:33:59.684690+00:00         273         273   1001   \n",
      "\n",
      "                                  measPeriodPdcpBytes  measPeriodPrb  \\\n",
      "2022-08-13 18:33:59.684690+00:00                   10             10   \n",
      "\n",
      "                                          measTimeStampRf  nrCellIdentity  \\\n",
      "2022-08-13 18:33:59.684690+00:00  2020-09-16T13:57:48.220               2   \n",
      "\n",
      "                                  pdcpBytesDl  pdcpBytesUl  \n",
      "2022-08-13 18:33:59.684690+00:00      5000000      5000000  \n",
      "self.data.index= DatetimeIndex(['2022-08-13 18:33:59.684690+00:00'], dtype='datetime64[ns, UTC]', freq=None)\n",
      "pd.to_numeric(self.data.index)= Int64Index([1660415639684690000], dtype='int64')\n",
      "self.data[measTimeStampRf]= 2022-08-13 18:33:59.684690+00:00    1660415639684690000\n",
      "Name: measTimeStampRf, dtype: int64\n",
      "////came back from db.DATABASE-read-data to connectdb.else///////\n",
      "///////enter def read_data(self, meas, limit=100): in class DATABASE///////////\n",
      "meas= cellMeas\n",
      "limit= 100\n",
      "str(limit)= 100\n",
      "self.client.get_list_database()= [{'name': '_internal'}, {'name': 'database_01.csv'}, {'name': 'database_05'}, {'name': 'antar2'}, {'name': 'mydb_01'}, {'name': 'CellData'}]\n",
      "self.client.get_list_measurements()= [{'name': 'cellMeas'}]\n",
      "result= defaultdict(<class 'list'>, {'cellMeas':                                   availPrbDl  availPrbUl  du-id  \\\n",
      "2022-08-13 18:33:59.684690+00:00         273         273   1001   \n",
      "\n",
      "                                  measPeriodPdcpBytes  measPeriodPrb  \\\n",
      "2022-08-13 18:33:59.684690+00:00                   10             10   \n",
      "\n",
      "                                          measTimeStampRf  nrCellIdentity  \\\n",
      "2022-08-13 18:33:59.684690+00:00  2020-09-16T13:57:48.220               2   \n",
      "\n",
      "                                  pdcpBytesDl  pdcpBytesUl  \n",
      "2022-08-13 18:33:59.684690+00:00      5000000      5000000  })\n",
      "Querying data : cellMeas : size - 1\n",
      "/////enter try in read-data def///\n",
      "result[meas]=                                   availPrbDl  availPrbUl  du-id  \\\n",
      "2022-08-13 18:33:59.684690+00:00         273         273   1001   \n",
      "\n",
      "                                  measPeriodPdcpBytes  measPeriodPrb  \\\n",
      "2022-08-13 18:33:59.684690+00:00                   10             10   \n",
      "\n",
      "                                          measTimeStampRf  nrCellIdentity  \\\n",
      "2022-08-13 18:33:59.684690+00:00  2020-09-16T13:57:48.220               2   \n",
      "\n",
      "                                  pdcpBytesDl  pdcpBytesUl  \n",
      "2022-08-13 18:33:59.684690+00:00      5000000      5000000  \n",
      "len(result[meas])= 1\n",
      "self.data==                                   availPrbDl  availPrbUl  du-id  \\\n",
      "2022-08-13 18:33:59.684690+00:00         273         273   1001   \n",
      "\n",
      "                                  measPeriodPdcpBytes  measPeriodPrb  \\\n",
      "2022-08-13 18:33:59.684690+00:00                   10             10   \n",
      "\n",
      "                                          measTimeStampRf  nrCellIdentity  \\\n",
      "2022-08-13 18:33:59.684690+00:00  2020-09-16T13:57:48.220               2   \n",
      "\n",
      "                                  pdcpBytesDl  pdcpBytesUl  \n",
      "2022-08-13 18:33:59.684690+00:00      5000000      5000000  \n",
      "self.data.index= DatetimeIndex(['2022-08-13 18:33:59.684690+00:00'], dtype='datetime64[ns, UTC]', freq=None)\n",
      "pd.to_numeric(self.data.index)= Int64Index([1660415639684690000], dtype='int64')\n",
      "self.data[measTimeStampRf]= 2022-08-13 18:33:59.684690+00:00    1660415639684690000\n",
      "Name: measTimeStampRf, dtype: int64\n",
      "db.read_data(\"cellMeas\")= None\n",
      "cell_data = db.data.values.tolist()= [[273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]]\n",
      "///////connectdb finished go to start//////\n",
      "///////come back from connectdb////////\n",
      "/////////////enter def load_model_parameters////////////////\n",
      "PATH = model.pth= model.pth\n",
      "cwd=os.getcwd= C:\\Users\\Mohammadreza\\Desktop\\My Class\\Proj-DC\\My Works\\Scheduling\\xApp\\mr7-main\\mr\n",
      "os.listdir(cwd)= ['.ipynb_checkpoints', 'assets', 'cells.csv', 'db.py', 'insert.py', 'keras_metadata.pb', 'main-changed-v3.ipynb', 'main-copy-v1.ipynb', 'main-Copy-v2.ipynb', 'main.py', 'model.pth', 'model.zip', 'mr_model', 'populate.py', 'saved_model.pb', 'sdl.py', 'ue.json.gz', 'valid.csv', 'variables', '__pycache__']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 9, 9)              396       \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 9, 16)             1664      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 32)                6272      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 9,454\n",
      "Trainable params: 9,454\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 9, 9)              396       \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 9, 16)             1664      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 32)                6272      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 9,454\n",
      "Trainable params: 9,454\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "model=LSTClassifier= None\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\Mohammadreza\\Desktop\\My Class\\Proj-DC\\My Works\\Scheduling\\xApp\\mr7-main\\mr\\assets\n",
      "ai_model.summary in start= <keras.engine.sequential.Sequential object at 0x000001EAFF3FB160>\n",
      "////////////enter def entry///////////////\n",
      "/////////pass 1 entry schedule.every(1).seconds.do(run_prediction, self)/////\n",
      "///////////////enter def run_prediction///////////////\n",
      "sample=[3735,...]= [3735, 0, 27648, 2295, 18, -1, 16383, -1, -1, -1]\n",
      "//////////enter if cell_data in run_prediction/////\n",
      "cell_data= [[273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]]\n",
      "pos=(pos + 1) % len(cell_data)= 0\n",
      "sample = cell_data[pos]= [273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]\n",
      "/////////////enter def predict/////////////////\n",
      "/////////////enter def load_model_parameters////////////////\n",
      "PATH = model.pth= model.pth\n",
      "cwd=os.getcwd= C:\\Users\\Mohammadreza\\Desktop\\My Class\\Proj-DC\\My Works\\Scheduling\\xApp\\mr7-main\\mr\n",
      "os.listdir(cwd)= ['.ipynb_checkpoints', 'assets', 'cells.csv', 'db.py', 'insert.py', 'keras_metadata.pb', 'main-changed-v3.ipynb', 'main-copy-v1.ipynb', 'main-Copy-v2.ipynb', 'main.py', 'model.pth', 'model.zip', 'mr_model', 'populate.py', 'saved_model.pb', 'sdl.py', 'ue.json.gz', 'valid.csv', 'variables', '__pycache__']\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 9, 9)              396       \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 9, 16)             1664      \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 32)                6272      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 9,454\n",
      "Trainable params: 9,454\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 9, 9)              396       \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 9, 16)             1664      \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 32)                6272      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 9,454\n",
      "Trainable params: 9,454\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "model=LSTClassifier= None\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\Mohammadreza\\Desktop\\My Class\\Proj-DC\\My Works\\Scheduling\\xApp\\mr7-main\\mr\\assets\n",
      "ai_model.summary()= <keras.engine.sequential.Sequential object at 0x000001EA891A59D0>\n",
      "celldata: [273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]\n",
      "/////////////////enter def predict_unseen_data///////////////////\n",
      "unseen_data= [273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]\n",
      "np_data= [2.7300000e+02 2.7300000e+02 1.0010000e+03 1.0000000e+01 1.0000000e+01\n",
      " 1.6604157e+18 2.0000000e+00 5.0000000e+06 5.0000000e+06]\n",
      "newaxis= None\n",
      "X_grouped = np_data[newaxis, newaxis, :]= [[[2.7300000e+02 2.7300000e+02 1.0010000e+03 1.0000000e+01 1.0000000e+01\n",
      "   1.6604157e+18 2.0000000e+00 5.0000000e+06 5.0000000e+06]]]\n",
      "X_grouped.transpose(0, 2, 1)= [[[2.7300000e+02]\n",
      "  [2.7300000e+02]\n",
      "  [1.0010000e+03]\n",
      "  [1.0000000e+01]\n",
      "  [1.0000000e+01]\n",
      "  [1.6604157e+18]\n",
      "  [2.0000000e+00]\n",
      "  [5.0000000e+06]\n",
      "  [5.0000000e+06]]]\n",
      "X_grouped.transpose= <built-in method transpose of numpy.ndarray object at 0x000001EAF6903270>\n",
      "X_grouped.transpose(0,2,1)= [[[2.7300000e+02]\n",
      "  [2.7300000e+02]\n",
      "  [1.0010000e+03]\n",
      "  [1.0000000e+01]\n",
      "  [1.0000000e+01]\n",
      "  [1.6604157e+18]\n",
      "  [2.0000000e+00]\n",
      "  [5.0000000e+06]\n",
      "  [5.0000000e+06]]]\n",
      "X_grouped= tf.Tensor(\n",
      "[[[2.7300000e+02]\n",
      "  [2.7300000e+02]\n",
      "  [1.0010000e+03]\n",
      "  [1.0000000e+01]\n",
      "  [1.0000000e+01]\n",
      "  [1.6604157e+18]\n",
      "  [2.0000000e+00]\n",
      "  [5.0000000e+06]\n",
      "  [5.0000000e+06]]], shape=(1, 9, 1), dtype=float32)\n",
      "y_fake= tf.Tensor([0], shape=(1,), dtype=int64)\n",
      "tensor_test= TensorDataset(X_grouped, y_fake)= <TensorDataset shapes: ((1, 9, 1), (1,)), types: (tf.float32, tf.int64)>\n",
      "test_dl= <BatchDataset shapes: ((None, 1, 9, 1), (None, 1)), types: (tf.float32, tf.int64)>\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step - loss: 17723953371119922502959104.0000\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 9ms/step - loss: nan\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 9ms/step - loss: nan\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 9ms/step - loss: nan\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 9ms/step - loss: nan\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: nan\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: nan\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: nan\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 8ms/step - loss: nan\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 8ms/step - loss: nan\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: nan\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 0s 9ms/step - loss: nan\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 0s 12ms/step - loss: nan\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 0s 9ms/step - loss: nan\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 0s 9ms/step - loss: nan\n",
      "///enter for loop in predict unseen data////\n",
      "batch= tf.Tensor(\n",
      "[[[[2.7300000e+02]\n",
      "   [2.7300000e+02]\n",
      "   [1.0010000e+03]\n",
      "   [1.0000000e+01]\n",
      "   [1.0000000e+01]\n",
      "   [1.6604157e+18]\n",
      "   [2.0000000e+00]\n",
      "   [5.0000000e+06]\n",
      "   [5.0000000e+06]]]], shape=(1, 1, 9, 1), dtype=float32)\n",
      "batch = batch[0]= tf.Tensor(\n",
      "[[[2.7300000e+02]\n",
      "  [2.7300000e+02]\n",
      "  [1.0010000e+03]\n",
      "  [1.0000000e+01]\n",
      "  [1.0000000e+01]\n",
      "  [1.6604157e+18]\n",
      "  [2.0000000e+00]\n",
      "  [5.0000000e+06]\n",
      "  [5.0000000e+06]]], shape=(1, 9, 1), dtype=float32)\n",
      "out=model(batch)= [[nan nan]]\n",
      "y_hat = tf.nn.log_softmax(out)= tf.Tensor([[nan nan]], shape=(1, 2), dtype=float32)\n",
      "y_hat = tf.argmax(y_hat)= tf.Tensor([0 0], shape=(2,), dtype=int64)\n",
      "y_hat = tf.get_static_value(y_hat, partial=False)= [0 0]\n",
      "ret += y_hat.tolist()= [0, 0]\n",
      "ret after for loop= [0, 0]\n",
      "ret[0] 0\n",
      "ret[0] == 0\n",
      "ret=predict_unseen_data(ai_model, celldata)= Normal\n",
      "celldata: [273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]\n",
      "Classification Normal\n",
      "////prediction doneee///////\n",
      "///////////////enter def run_prediction///////////////\n",
      "sample=[3735,...]= [3735, 0, 27648, 2295, 18, -1, 16383, -1, -1, -1]\n",
      "//////////enter if cell_data in run_prediction/////\n",
      "cell_data= [[273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]]\n",
      "pos=(pos + 1) % len(cell_data)= 0\n",
      "sample = cell_data[pos]= [273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]\n",
      "/////////////enter def predict/////////////////\n",
      "/////////////enter def load_model_parameters////////////////\n",
      "PATH = model.pth= model.pth\n",
      "cwd=os.getcwd= C:\\Users\\Mohammadreza\\Desktop\\My Class\\Proj-DC\\My Works\\Scheduling\\xApp\\mr7-main\\mr\n",
      "os.listdir(cwd)= ['.ipynb_checkpoints', 'assets', 'cells.csv', 'db.py', 'insert.py', 'keras_metadata.pb', 'main-changed-v3.ipynb', 'main-copy-v1.ipynb', 'main-Copy-v2.ipynb', 'main.py', 'model.pth', 'model.zip', 'mr_model', 'populate.py', 'saved_model.pb', 'sdl.py', 'ue.json.gz', 'valid.csv', 'variables', '__pycache__']\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_6 (LSTM)                (None, 9, 9)              396       \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 9, 16)             1664      \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 32)                6272      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 9,454\n",
      "Trainable params: 9,454\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_6 (LSTM)                (None, 9, 9)              396       \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 9, 16)             1664      \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 32)                6272      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 9,454\n",
      "Trainable params: 9,454\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "model=LSTClassifier= None\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\Mohammadreza\\Desktop\\My Class\\Proj-DC\\My Works\\Scheduling\\xApp\\mr7-main\\mr\\assets\n",
      "ai_model.summary()= <keras.engine.sequential.Sequential object at 0x000001EAFDE0D310>\n",
      "celldata: [273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]\n",
      "/////////////////enter def predict_unseen_data///////////////////\n",
      "unseen_data= [273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]\n",
      "np_data= [2.7300000e+02 2.7300000e+02 1.0010000e+03 1.0000000e+01 1.0000000e+01\n",
      " 1.6604157e+18 2.0000000e+00 5.0000000e+06 5.0000000e+06]\n",
      "newaxis= None\n",
      "X_grouped = np_data[newaxis, newaxis, :]= [[[2.7300000e+02 2.7300000e+02 1.0010000e+03 1.0000000e+01 1.0000000e+01\n",
      "   1.6604157e+18 2.0000000e+00 5.0000000e+06 5.0000000e+06]]]\n",
      "X_grouped.transpose(0, 2, 1)= [[[2.7300000e+02]\n",
      "  [2.7300000e+02]\n",
      "  [1.0010000e+03]\n",
      "  [1.0000000e+01]\n",
      "  [1.0000000e+01]\n",
      "  [1.6604157e+18]\n",
      "  [2.0000000e+00]\n",
      "  [5.0000000e+06]\n",
      "  [5.0000000e+06]]]\n",
      "X_grouped.transpose= <built-in method transpose of numpy.ndarray object at 0x000001EA8B5514B0>\n",
      "X_grouped.transpose(0,2,1)= [[[2.7300000e+02]\n",
      "  [2.7300000e+02]\n",
      "  [1.0010000e+03]\n",
      "  [1.0000000e+01]\n",
      "  [1.0000000e+01]\n",
      "  [1.6604157e+18]\n",
      "  [2.0000000e+00]\n",
      "  [5.0000000e+06]\n",
      "  [5.0000000e+06]]]\n",
      "X_grouped= tf.Tensor(\n",
      "[[[2.7300000e+02]\n",
      "  [2.7300000e+02]\n",
      "  [1.0010000e+03]\n",
      "  [1.0000000e+01]\n",
      "  [1.0000000e+01]\n",
      "  [1.6604157e+18]\n",
      "  [2.0000000e+00]\n",
      "  [5.0000000e+06]\n",
      "  [5.0000000e+06]]], shape=(1, 9, 1), dtype=float32)\n",
      "y_fake= tf.Tensor([0], shape=(1,), dtype=int64)\n",
      "tensor_test= TensorDataset(X_grouped, y_fake)= <TensorDataset shapes: ((1, 9, 1), (1,)), types: (tf.float32, tf.int64)>\n",
      "test_dl= <BatchDataset shapes: ((None, 1, 9, 1), (None, 1)), types: (tf.float32, tf.int64)>\n",
      "Epoch 1/20\n",
      "1/1 [==============================] - 1s 1s/step - loss: 554028729693687235267961001345024.0000\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 12ms/step - loss: nan\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 12ms/step - loss: nan\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 9ms/step - loss: nan\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 12ms/step - loss: nan\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 0s 9ms/step - loss: nan\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 0s 8ms/step - loss: nan\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: nan\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: nan\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 0s 8ms/step - loss: nan\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 0s 8ms/step - loss: nan\n",
      "///enter for loop in predict unseen data////\n",
      "batch= tf.Tensor(\n",
      "[[[[2.7300000e+02]\n",
      "   [2.7300000e+02]\n",
      "   [1.0010000e+03]\n",
      "   [1.0000000e+01]\n",
      "   [1.0000000e+01]\n",
      "   [1.6604157e+18]\n",
      "   [2.0000000e+00]\n",
      "   [5.0000000e+06]\n",
      "   [5.0000000e+06]]]], shape=(1, 1, 9, 1), dtype=float32)\n",
      "batch = batch[0]= tf.Tensor(\n",
      "[[[2.7300000e+02]\n",
      "  [2.7300000e+02]\n",
      "  [1.0010000e+03]\n",
      "  [1.0000000e+01]\n",
      "  [1.0000000e+01]\n",
      "  [1.6604157e+18]\n",
      "  [2.0000000e+00]\n",
      "  [5.0000000e+06]\n",
      "  [5.0000000e+06]]], shape=(1, 9, 1), dtype=float32)\n",
      "out=model(batch)= [[nan nan]]\n",
      "y_hat = tf.nn.log_softmax(out)= tf.Tensor([[nan nan]], shape=(1, 2), dtype=float32)\n",
      "y_hat = tf.argmax(y_hat)= tf.Tensor([0 0], shape=(2,), dtype=int64)\n",
      "y_hat = tf.get_static_value(y_hat, partial=False)= [0 0]\n",
      "ret += y_hat.tolist()= [0, 0]\n",
      "ret after for loop= [0, 0]\n",
      "ret[0] 0\n",
      "ret[0] == 0\n",
      "ret=predict_unseen_data(ai_model, celldata)= Normal\n",
      "celldata: [273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]\n",
      "Classification Normal\n",
      "////prediction doneee///////\n",
      "///////////////enter def run_prediction///////////////\n",
      "sample=[3735,...]= [3735, 0, 27648, 2295, 18, -1, 16383, -1, -1, -1]\n",
      "//////////enter if cell_data in run_prediction/////\n",
      "cell_data= [[273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]]\n",
      "pos=(pos + 1) % len(cell_data)= 0\n",
      "sample = cell_data[pos]= [273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]\n",
      "/////////////enter def predict/////////////////\n",
      "/////////////enter def load_model_parameters////////////////\n",
      "PATH = model.pth= model.pth\n",
      "cwd=os.getcwd= C:\\Users\\Mohammadreza\\Desktop\\My Class\\Proj-DC\\My Works\\Scheduling\\xApp\\mr7-main\\mr\n",
      "os.listdir(cwd)= ['.ipynb_checkpoints', 'assets', 'cells.csv', 'db.py', 'insert.py', 'keras_metadata.pb', 'main-changed-v3.ipynb', 'main-copy-v1.ipynb', 'main-Copy-v2.ipynb', 'main.py', 'model.pth', 'model.zip', 'mr_model', 'populate.py', 'saved_model.pb', 'sdl.py', 'ue.json.gz', 'valid.csv', 'variables', '__pycache__']\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_9 (LSTM)                (None, 9, 9)              396       \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 9, 16)             1664      \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 32)                6272      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 9,454\n",
      "Trainable params: 9,454\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_9 (LSTM)                (None, 9, 9)              396       \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 9, 16)             1664      \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 32)                6272      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 9,454\n",
      "Trainable params: 9,454\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "model=LSTClassifier= None\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\Mohammadreza\\Desktop\\My Class\\Proj-DC\\My Works\\Scheduling\\xApp\\mr7-main\\mr\\assets\n",
      "ai_model.summary()= <keras.engine.sequential.Sequential object at 0x000001EA94027EB0>\n",
      "celldata: [273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]\n",
      "/////////////////enter def predict_unseen_data///////////////////\n",
      "unseen_data= [273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]\n",
      "np_data= [2.7300000e+02 2.7300000e+02 1.0010000e+03 1.0000000e+01 1.0000000e+01\n",
      " 1.6604157e+18 2.0000000e+00 5.0000000e+06 5.0000000e+06]\n",
      "newaxis= None\n",
      "X_grouped = np_data[newaxis, newaxis, :]= [[[2.7300000e+02 2.7300000e+02 1.0010000e+03 1.0000000e+01 1.0000000e+01\n",
      "   1.6604157e+18 2.0000000e+00 5.0000000e+06 5.0000000e+06]]]\n",
      "X_grouped.transpose(0, 2, 1)= [[[2.7300000e+02]\n",
      "  [2.7300000e+02]\n",
      "  [1.0010000e+03]\n",
      "  [1.0000000e+01]\n",
      "  [1.0000000e+01]\n",
      "  [1.6604157e+18]\n",
      "  [2.0000000e+00]\n",
      "  [5.0000000e+06]\n",
      "  [5.0000000e+06]]]\n",
      "X_grouped.transpose= <built-in method transpose of numpy.ndarray object at 0x000001EAFFAC9570>\n",
      "X_grouped.transpose(0,2,1)= [[[2.7300000e+02]\n",
      "  [2.7300000e+02]\n",
      "  [1.0010000e+03]\n",
      "  [1.0000000e+01]\n",
      "  [1.0000000e+01]\n",
      "  [1.6604157e+18]\n",
      "  [2.0000000e+00]\n",
      "  [5.0000000e+06]\n",
      "  [5.0000000e+06]]]\n",
      "X_grouped= tf.Tensor(\n",
      "[[[2.7300000e+02]\n",
      "  [2.7300000e+02]\n",
      "  [1.0010000e+03]\n",
      "  [1.0000000e+01]\n",
      "  [1.0000000e+01]\n",
      "  [1.6604157e+18]\n",
      "  [2.0000000e+00]\n",
      "  [5.0000000e+06]\n",
      "  [5.0000000e+06]]], shape=(1, 9, 1), dtype=float32)\n",
      "y_fake= tf.Tensor([0], shape=(1,), dtype=int64)\n",
      "tensor_test= TensorDataset(X_grouped, y_fake)= <TensorDataset shapes: ((1, 9, 1), (1,)), types: (tf.float32, tf.int64)>\n",
      "test_dl= <BatchDataset shapes: ((None, 1, 9, 1), (None, 1)), types: (tf.float32, tf.int64)>\n",
      "Epoch 1/20\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.4658e-07\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.5755e-09\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 8.2042e-08\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 7.8168e-08\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.9821e-08\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.7634e-09\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.5510e-08\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.7756e-08\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 4.0817e-08\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.3265e-08\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 10ms/step - loss: 4.7007e-09\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.2604e-09\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.1937e-08\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.2092e-08\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.0615e-08\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.0233e-08\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.3506e-09\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.0430e-09\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 7.2343e-09\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.2096e-08\n",
      "///enter for loop in predict unseen data////\n",
      "batch= tf.Tensor(\n",
      "[[[[2.7300000e+02]\n",
      "   [2.7300000e+02]\n",
      "   [1.0010000e+03]\n",
      "   [1.0000000e+01]\n",
      "   [1.0000000e+01]\n",
      "   [1.6604157e+18]\n",
      "   [2.0000000e+00]\n",
      "   [5.0000000e+06]\n",
      "   [5.0000000e+06]]]], shape=(1, 1, 9, 1), dtype=float32)\n",
      "batch = batch[0]= tf.Tensor(\n",
      "[[[2.7300000e+02]\n",
      "  [2.7300000e+02]\n",
      "  [1.0010000e+03]\n",
      "  [1.0000000e+01]\n",
      "  [1.0000000e+01]\n",
      "  [1.6604157e+18]\n",
      "  [2.0000000e+00]\n",
      "  [5.0000000e+06]\n",
      "  [5.0000000e+06]]], shape=(1, 9, 1), dtype=float32)\n",
      "out=model(batch)= [[-5.2297317e-05 -1.3655746e-04]]\n",
      "y_hat = tf.nn.log_softmax(out)= tf.Tensor([[-0.69310504 -0.6931893 ]], shape=(1, 2), dtype=float32)\n",
      "y_hat = tf.argmax(y_hat)= tf.Tensor([0 0], shape=(2,), dtype=int64)\n",
      "y_hat = tf.get_static_value(y_hat, partial=False)= [0 0]\n",
      "ret += y_hat.tolist()= [0, 0]\n",
      "ret after for loop= [0, 0]\n",
      "ret[0] 0\n",
      "ret[0] == 0\n",
      "ret=predict_unseen_data(ai_model, celldata)= Normal\n",
      "celldata: [273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]\n",
      "Classification Normal\n",
      "////prediction doneee///////\n",
      "///////////////enter def run_prediction///////////////\n",
      "sample=[3735,...]= [3735, 0, 27648, 2295, 18, -1, 16383, -1, -1, -1]\n",
      "//////////enter if cell_data in run_prediction/////\n",
      "cell_data= [[273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]]\n",
      "pos=(pos + 1) % len(cell_data)= 0\n",
      "sample = cell_data[pos]= [273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]\n",
      "/////////////enter def predict/////////////////\n",
      "/////////////enter def load_model_parameters////////////////\n",
      "PATH = model.pth= model.pth\n",
      "cwd=os.getcwd= C:\\Users\\Mohammadreza\\Desktop\\My Class\\Proj-DC\\My Works\\Scheduling\\xApp\\mr7-main\\mr\n",
      "os.listdir(cwd)= ['.ipynb_checkpoints', 'assets', 'cells.csv', 'db.py', 'insert.py', 'keras_metadata.pb', 'main-changed-v3.ipynb', 'main-copy-v1.ipynb', 'main-Copy-v2.ipynb', 'main.py', 'model.pth', 'model.zip', 'mr_model', 'populate.py', 'saved_model.pb', 'sdl.py', 'ue.json.gz', 'valid.csv', 'variables', '__pycache__']\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_12 (LSTM)               (None, 9, 9)              396       \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 9, 16)             1664      \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 32)                6272      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 9,454\n",
      "Trainable params: 9,454\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_12 (LSTM)               (None, 9, 9)              396       \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 9, 16)             1664      \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 32)                6272      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 9,454\n",
      "Trainable params: 9,454\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "model=LSTClassifier= None\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\Mohammadreza\\Desktop\\My Class\\Proj-DC\\My Works\\Scheduling\\xApp\\mr7-main\\mr\\assets\n",
      "ai_model.summary()= <keras.engine.sequential.Sequential object at 0x000001EA950E3F10>\n",
      "celldata: [273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]\n",
      "/////////////////enter def predict_unseen_data///////////////////\n",
      "unseen_data= [273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]\n",
      "np_data= [2.7300000e+02 2.7300000e+02 1.0010000e+03 1.0000000e+01 1.0000000e+01\n",
      " 1.6604157e+18 2.0000000e+00 5.0000000e+06 5.0000000e+06]\n",
      "newaxis= None\n",
      "X_grouped = np_data[newaxis, newaxis, :]= [[[2.7300000e+02 2.7300000e+02 1.0010000e+03 1.0000000e+01 1.0000000e+01\n",
      "   1.6604157e+18 2.0000000e+00 5.0000000e+06 5.0000000e+06]]]\n",
      "X_grouped.transpose(0, 2, 1)= [[[2.7300000e+02]\n",
      "  [2.7300000e+02]\n",
      "  [1.0010000e+03]\n",
      "  [1.0000000e+01]\n",
      "  [1.0000000e+01]\n",
      "  [1.6604157e+18]\n",
      "  [2.0000000e+00]\n",
      "  [5.0000000e+06]\n",
      "  [5.0000000e+06]]]\n",
      "X_grouped.transpose= <built-in method transpose of numpy.ndarray object at 0x000001EA88054ED0>\n",
      "X_grouped.transpose(0,2,1)= [[[2.7300000e+02]\n",
      "  [2.7300000e+02]\n",
      "  [1.0010000e+03]\n",
      "  [1.0000000e+01]\n",
      "  [1.0000000e+01]\n",
      "  [1.6604157e+18]\n",
      "  [2.0000000e+00]\n",
      "  [5.0000000e+06]\n",
      "  [5.0000000e+06]]]\n",
      "X_grouped= tf.Tensor(\n",
      "[[[2.7300000e+02]\n",
      "  [2.7300000e+02]\n",
      "  [1.0010000e+03]\n",
      "  [1.0000000e+01]\n",
      "  [1.0000000e+01]\n",
      "  [1.6604157e+18]\n",
      "  [2.0000000e+00]\n",
      "  [5.0000000e+06]\n",
      "  [5.0000000e+06]]], shape=(1, 9, 1), dtype=float32)\n",
      "y_fake= tf.Tensor([0], shape=(1,), dtype=int64)\n",
      "tensor_test= TensorDataset(X_grouped, y_fake)= <TensorDataset shapes: ((1, 9, 1), (1,)), types: (tf.float32, tf.int64)>\n",
      "test_dl= <BatchDataset shapes: ((None, 1, 9, 1), (None, 1)), types: (tf.float32, tf.int64)>\n",
      "Epoch 1/20\n",
      "1/1 [==============================] - 2s 2s/step - loss: 808436500870153665597038133248.0000\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 12ms/step - loss: nan\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 12ms/step - loss: nan\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 12ms/step - loss: nan\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 12ms/step - loss: nan\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 12ms/step - loss: nan\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 9ms/step - loss: nan\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 0s 12ms/step - loss: nan\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: nan\n",
      "///enter for loop in predict unseen data////\n",
      "batch= tf.Tensor(\n",
      "[[[[2.7300000e+02]\n",
      "   [2.7300000e+02]\n",
      "   [1.0010000e+03]\n",
      "   [1.0000000e+01]\n",
      "   [1.0000000e+01]\n",
      "   [1.6604157e+18]\n",
      "   [2.0000000e+00]\n",
      "   [5.0000000e+06]\n",
      "   [5.0000000e+06]]]], shape=(1, 1, 9, 1), dtype=float32)\n",
      "batch = batch[0]= tf.Tensor(\n",
      "[[[2.7300000e+02]\n",
      "  [2.7300000e+02]\n",
      "  [1.0010000e+03]\n",
      "  [1.0000000e+01]\n",
      "  [1.0000000e+01]\n",
      "  [1.6604157e+18]\n",
      "  [2.0000000e+00]\n",
      "  [5.0000000e+06]\n",
      "  [5.0000000e+06]]], shape=(1, 9, 1), dtype=float32)\n",
      "out=model(batch)= [[nan nan]]\n",
      "y_hat = tf.nn.log_softmax(out)= tf.Tensor([[nan nan]], shape=(1, 2), dtype=float32)\n",
      "y_hat = tf.argmax(y_hat)= tf.Tensor([0 0], shape=(2,), dtype=int64)\n",
      "y_hat = tf.get_static_value(y_hat, partial=False)= [0 0]\n",
      "ret += y_hat.tolist()= [0, 0]\n",
      "ret after for loop= [0, 0]\n",
      "ret[0] 0\n",
      "ret[0] == 0\n",
      "ret=predict_unseen_data(ai_model, celldata)= Normal\n",
      "celldata: [273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]\n",
      "Classification Normal\n",
      "////prediction doneee///////\n",
      "///////////////enter def run_prediction///////////////\n",
      "sample=[3735,...]= [3735, 0, 27648, 2295, 18, -1, 16383, -1, -1, -1]\n",
      "//////////enter if cell_data in run_prediction/////\n",
      "cell_data= [[273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]]\n",
      "pos=(pos + 1) % len(cell_data)= 0\n",
      "sample = cell_data[pos]= [273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]\n",
      "/////////////enter def predict/////////////////\n",
      "/////////////enter def load_model_parameters////////////////\n",
      "PATH = model.pth= model.pth\n",
      "cwd=os.getcwd= C:\\Users\\Mohammadreza\\Desktop\\My Class\\Proj-DC\\My Works\\Scheduling\\xApp\\mr7-main\\mr\n",
      "os.listdir(cwd)= ['.ipynb_checkpoints', 'assets', 'cells.csv', 'db.py', 'insert.py', 'keras_metadata.pb', 'main-changed-v3.ipynb', 'main-copy-v1.ipynb', 'main-Copy-v2.ipynb', 'main.py', 'model.pth', 'model.zip', 'mr_model', 'populate.py', 'saved_model.pb', 'sdl.py', 'ue.json.gz', 'valid.csv', 'variables', '__pycache__']\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_15 (LSTM)               (None, 9, 9)              396       \n",
      "_________________________________________________________________\n",
      "lstm_16 (LSTM)               (None, 9, 16)             1664      \n",
      "_________________________________________________________________\n",
      "lstm_17 (LSTM)               (None, 32)                6272      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 9,454\n",
      "Trainable params: 9,454\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_15 (LSTM)               (None, 9, 9)              396       \n",
      "_________________________________________________________________\n",
      "lstm_16 (LSTM)               (None, 9, 16)             1664      \n",
      "_________________________________________________________________\n",
      "lstm_17 (LSTM)               (None, 32)                6272      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 9,454\n",
      "Trainable params: 9,454\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "model=LSTClassifier= None\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\Mohammadreza\\Desktop\\My Class\\Proj-DC\\My Works\\Scheduling\\xApp\\mr7-main\\mr\\assets\n",
      "ai_model.summary()= <keras.engine.sequential.Sequential object at 0x000001EA8E9F5F70>\n",
      "celldata: [273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]\n",
      "/////////////////enter def predict_unseen_data///////////////////\n",
      "unseen_data= [273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]\n",
      "np_data= [2.7300000e+02 2.7300000e+02 1.0010000e+03 1.0000000e+01 1.0000000e+01\n",
      " 1.6604157e+18 2.0000000e+00 5.0000000e+06 5.0000000e+06]\n",
      "newaxis= None\n",
      "X_grouped = np_data[newaxis, newaxis, :]= [[[2.7300000e+02 2.7300000e+02 1.0010000e+03 1.0000000e+01 1.0000000e+01\n",
      "   1.6604157e+18 2.0000000e+00 5.0000000e+06 5.0000000e+06]]]\n",
      "X_grouped.transpose(0, 2, 1)= [[[2.7300000e+02]\n",
      "  [2.7300000e+02]\n",
      "  [1.0010000e+03]\n",
      "  [1.0000000e+01]\n",
      "  [1.0000000e+01]\n",
      "  [1.6604157e+18]\n",
      "  [2.0000000e+00]\n",
      "  [5.0000000e+06]\n",
      "  [5.0000000e+06]]]\n",
      "X_grouped.transpose= <built-in method transpose of numpy.ndarray object at 0x000001EA983DDE10>\n",
      "X_grouped.transpose(0,2,1)= [[[2.7300000e+02]\n",
      "  [2.7300000e+02]\n",
      "  [1.0010000e+03]\n",
      "  [1.0000000e+01]\n",
      "  [1.0000000e+01]\n",
      "  [1.6604157e+18]\n",
      "  [2.0000000e+00]\n",
      "  [5.0000000e+06]\n",
      "  [5.0000000e+06]]]\n",
      "X_grouped= tf.Tensor(\n",
      "[[[2.7300000e+02]\n",
      "  [2.7300000e+02]\n",
      "  [1.0010000e+03]\n",
      "  [1.0000000e+01]\n",
      "  [1.0000000e+01]\n",
      "  [1.6604157e+18]\n",
      "  [2.0000000e+00]\n",
      "  [5.0000000e+06]\n",
      "  [5.0000000e+06]]], shape=(1, 9, 1), dtype=float32)\n",
      "y_fake= tf.Tensor([0], shape=(1,), dtype=int64)\n",
      "tensor_test= TensorDataset(X_grouped, y_fake)= <TensorDataset shapes: ((1, 9, 1), (1,)), types: (tf.float32, tf.int64)>\n",
      "test_dl= <BatchDataset shapes: ((None, 1, 9, 1), (None, 1)), types: (tf.float32, tf.int64)>\n",
      "Epoch 1/20\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1348342016331409795877582143488.0000\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 12ms/step - loss: nan\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 9ms/step - loss: nan\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 9ms/step - loss: nan\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 12ms/step - loss: nan\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 0s 9ms/step - loss: nan\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 0s 8ms/step - loss: nan\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 0s 12ms/step - loss: nan\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 0s 12ms/step - loss: nan\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 8ms/step - loss: nan\n",
      "///enter for loop in predict unseen data////\n",
      "batch= tf.Tensor(\n",
      "[[[[2.7300000e+02]\n",
      "   [2.7300000e+02]\n",
      "   [1.0010000e+03]\n",
      "   [1.0000000e+01]\n",
      "   [1.0000000e+01]\n",
      "   [1.6604157e+18]\n",
      "   [2.0000000e+00]\n",
      "   [5.0000000e+06]\n",
      "   [5.0000000e+06]]]], shape=(1, 1, 9, 1), dtype=float32)\n",
      "batch = batch[0]= tf.Tensor(\n",
      "[[[2.7300000e+02]\n",
      "  [2.7300000e+02]\n",
      "  [1.0010000e+03]\n",
      "  [1.0000000e+01]\n",
      "  [1.0000000e+01]\n",
      "  [1.6604157e+18]\n",
      "  [2.0000000e+00]\n",
      "  [5.0000000e+06]\n",
      "  [5.0000000e+06]]], shape=(1, 9, 1), dtype=float32)\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001EAFF231040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "out=model(batch)= [[nan nan]]\n",
      "y_hat = tf.nn.log_softmax(out)= tf.Tensor([[nan nan]], shape=(1, 2), dtype=float32)\n",
      "y_hat = tf.argmax(y_hat)= tf.Tensor([0 0], shape=(2,), dtype=int64)\n",
      "y_hat = tf.get_static_value(y_hat, partial=False)= [0 0]\n",
      "ret += y_hat.tolist()= [0, 0]\n",
      "ret after for loop= [0, 0]\n",
      "ret[0] 0\n",
      "ret[0] == 0\n",
      "ret=predict_unseen_data(ai_model, celldata)= Normal\n",
      "celldata: [273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]\n",
      "Classification Normal\n",
      "////prediction doneee///////\n",
      "///////////////enter def run_prediction///////////////\n",
      "sample=[3735,...]= [3735, 0, 27648, 2295, 18, -1, 16383, -1, -1, -1]\n",
      "//////////enter if cell_data in run_prediction/////\n",
      "cell_data= [[273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]]\n",
      "pos=(pos + 1) % len(cell_data)= 0\n",
      "sample = cell_data[pos]= [273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]\n",
      "/////////////enter def predict/////////////////\n",
      "/////////////enter def load_model_parameters////////////////\n",
      "PATH = model.pth= model.pth\n",
      "cwd=os.getcwd= C:\\Users\\Mohammadreza\\Desktop\\My Class\\Proj-DC\\My Works\\Scheduling\\xApp\\mr7-main\\mr\n",
      "os.listdir(cwd)= ['.ipynb_checkpoints', 'assets', 'cells.csv', 'db.py', 'insert.py', 'keras_metadata.pb', 'main-changed-v3.ipynb', 'main-copy-v1.ipynb', 'main-Copy-v2.ipynb', 'main.py', 'model.pth', 'model.zip', 'mr_model', 'populate.py', 'saved_model.pb', 'sdl.py', 'ue.json.gz', 'valid.csv', 'variables', '__pycache__']\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_18 (LSTM)               (None, 9, 9)              396       \n",
      "_________________________________________________________________\n",
      "lstm_19 (LSTM)               (None, 9, 16)             1664      \n",
      "_________________________________________________________________\n",
      "lstm_20 (LSTM)               (None, 32)                6272      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 9,454\n",
      "Trainable params: 9,454\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_18 (LSTM)               (None, 9, 9)              396       \n",
      "_________________________________________________________________\n",
      "lstm_19 (LSTM)               (None, 9, 16)             1664      \n",
      "_________________________________________________________________\n",
      "lstm_20 (LSTM)               (None, 32)                6272      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 9,454\n",
      "Trainable params: 9,454\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "model=LSTClassifier= None\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\Mohammadreza\\Desktop\\My Class\\Proj-DC\\My Works\\Scheduling\\xApp\\mr7-main\\mr\\assets\n",
      "ai_model.summary()= <keras.engine.sequential.Sequential object at 0x000001EA8B5F0E50>\n",
      "celldata: [273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]\n",
      "/////////////////enter def predict_unseen_data///////////////////\n",
      "unseen_data= [273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]\n",
      "np_data= [2.7300000e+02 2.7300000e+02 1.0010000e+03 1.0000000e+01 1.0000000e+01\n",
      " 1.6604157e+18 2.0000000e+00 5.0000000e+06 5.0000000e+06]\n",
      "newaxis= None\n",
      "X_grouped = np_data[newaxis, newaxis, :]= [[[2.7300000e+02 2.7300000e+02 1.0010000e+03 1.0000000e+01 1.0000000e+01\n",
      "   1.6604157e+18 2.0000000e+00 5.0000000e+06 5.0000000e+06]]]\n",
      "X_grouped.transpose(0, 2, 1)= [[[2.7300000e+02]\n",
      "  [2.7300000e+02]\n",
      "  [1.0010000e+03]\n",
      "  [1.0000000e+01]\n",
      "  [1.0000000e+01]\n",
      "  [1.6604157e+18]\n",
      "  [2.0000000e+00]\n",
      "  [5.0000000e+06]\n",
      "  [5.0000000e+06]]]\n",
      "X_grouped.transpose= <built-in method transpose of numpy.ndarray object at 0x000001EA972A9A50>\n",
      "X_grouped.transpose(0,2,1)= [[[2.7300000e+02]\n",
      "  [2.7300000e+02]\n",
      "  [1.0010000e+03]\n",
      "  [1.0000000e+01]\n",
      "  [1.0000000e+01]\n",
      "  [1.6604157e+18]\n",
      "  [2.0000000e+00]\n",
      "  [5.0000000e+06]\n",
      "  [5.0000000e+06]]]\n",
      "X_grouped= tf.Tensor(\n",
      "[[[2.7300000e+02]\n",
      "  [2.7300000e+02]\n",
      "  [1.0010000e+03]\n",
      "  [1.0000000e+01]\n",
      "  [1.0000000e+01]\n",
      "  [1.6604157e+18]\n",
      "  [2.0000000e+00]\n",
      "  [5.0000000e+06]\n",
      "  [5.0000000e+06]]], shape=(1, 9, 1), dtype=float32)\n",
      "y_fake= tf.Tensor([0], shape=(1,), dtype=int64)\n",
      "tensor_test= TensorDataset(X_grouped, y_fake)= <TensorDataset shapes: ((1, 9, 1), (1,)), types: (tf.float32, tf.int64)>\n",
      "test_dl= <BatchDataset shapes: ((None, 1, 9, 1), (None, 1)), types: (tf.float32, tf.int64)>\n",
      "Epoch 1/20\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1212283308847153903581628727296.0000\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 12ms/step - loss: nan\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 12ms/step - loss: nan\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 12ms/step - loss: nan\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 9ms/step - loss: nan\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 0s 12ms/step - loss: nan\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 0s 8ms/step - loss: nan\n",
      "///enter for loop in predict unseen data////\n",
      "batch= tf.Tensor(\n",
      "[[[[2.7300000e+02]\n",
      "   [2.7300000e+02]\n",
      "   [1.0010000e+03]\n",
      "   [1.0000000e+01]\n",
      "   [1.0000000e+01]\n",
      "   [1.6604157e+18]\n",
      "   [2.0000000e+00]\n",
      "   [5.0000000e+06]\n",
      "   [5.0000000e+06]]]], shape=(1, 1, 9, 1), dtype=float32)\n",
      "batch = batch[0]= tf.Tensor(\n",
      "[[[2.7300000e+02]\n",
      "  [2.7300000e+02]\n",
      "  [1.0010000e+03]\n",
      "  [1.0000000e+01]\n",
      "  [1.0000000e+01]\n",
      "  [1.6604157e+18]\n",
      "  [2.0000000e+00]\n",
      "  [5.0000000e+06]\n",
      "  [5.0000000e+06]]], shape=(1, 9, 1), dtype=float32)\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001EAFF269D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "out=model(batch)= [[nan nan]]\n",
      "y_hat = tf.nn.log_softmax(out)= tf.Tensor([[nan nan]], shape=(1, 2), dtype=float32)\n",
      "y_hat = tf.argmax(y_hat)= tf.Tensor([0 0], shape=(2,), dtype=int64)\n",
      "y_hat = tf.get_static_value(y_hat, partial=False)= [0 0]\n",
      "ret += y_hat.tolist()= [0, 0]\n",
      "ret after for loop= [0, 0]\n",
      "ret[0] 0\n",
      "ret[0] == 0\n",
      "ret=predict_unseen_data(ai_model, celldata)= Normal\n",
      "celldata: [273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]\n",
      "Classification Normal\n",
      "////prediction doneee///////\n",
      "///////////////enter def run_prediction///////////////\n",
      "sample=[3735,...]= [3735, 0, 27648, 2295, 18, -1, 16383, -1, -1, -1]\n",
      "//////////enter if cell_data in run_prediction/////\n",
      "cell_data= [[273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]]\n",
      "pos=(pos + 1) % len(cell_data)= 0\n",
      "sample = cell_data[pos]= [273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]\n",
      "/////////////enter def predict/////////////////\n",
      "/////////////enter def load_model_parameters////////////////\n",
      "PATH = model.pth= model.pth\n",
      "cwd=os.getcwd= C:\\Users\\Mohammadreza\\Desktop\\My Class\\Proj-DC\\My Works\\Scheduling\\xApp\\mr7-main\\mr\n",
      "os.listdir(cwd)= ['.ipynb_checkpoints', 'assets', 'cells.csv', 'db.py', 'insert.py', 'keras_metadata.pb', 'main-changed-v3.ipynb', 'main-copy-v1.ipynb', 'main-Copy-v2.ipynb', 'main.py', 'model.pth', 'model.zip', 'mr_model', 'populate.py', 'saved_model.pb', 'sdl.py', 'ue.json.gz', 'valid.csv', 'variables', '__pycache__']\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_21 (LSTM)               (None, 9, 9)              396       \n",
      "_________________________________________________________________\n",
      "lstm_22 (LSTM)               (None, 9, 16)             1664      \n",
      "_________________________________________________________________\n",
      "lstm_23 (LSTM)               (None, 32)                6272      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 9,454\n",
      "Trainable params: 9,454\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_21 (LSTM)               (None, 9, 9)              396       \n",
      "_________________________________________________________________\n",
      "lstm_22 (LSTM)               (None, 9, 16)             1664      \n",
      "_________________________________________________________________\n",
      "lstm_23 (LSTM)               (None, 32)                6272      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 9,454\n",
      "Trainable params: 9,454\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "model=LSTClassifier= None\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\Mohammadreza\\Desktop\\My Class\\Proj-DC\\My Works\\Scheduling\\xApp\\mr7-main\\mr\\assets\n",
      "ai_model.summary()= <keras.engine.sequential.Sequential object at 0x000001EA8C696F40>\n",
      "celldata: [273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]\n",
      "/////////////////enter def predict_unseen_data///////////////////\n",
      "unseen_data= [273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]\n",
      "np_data= [2.7300000e+02 2.7300000e+02 1.0010000e+03 1.0000000e+01 1.0000000e+01\n",
      " 1.6604157e+18 2.0000000e+00 5.0000000e+06 5.0000000e+06]\n",
      "newaxis= None\n",
      "X_grouped = np_data[newaxis, newaxis, :]= [[[2.7300000e+02 2.7300000e+02 1.0010000e+03 1.0000000e+01 1.0000000e+01\n",
      "   1.6604157e+18 2.0000000e+00 5.0000000e+06 5.0000000e+06]]]\n",
      "X_grouped.transpose(0, 2, 1)= [[[2.7300000e+02]\n",
      "  [2.7300000e+02]\n",
      "  [1.0010000e+03]\n",
      "  [1.0000000e+01]\n",
      "  [1.0000000e+01]\n",
      "  [1.6604157e+18]\n",
      "  [2.0000000e+00]\n",
      "  [5.0000000e+06]\n",
      "  [5.0000000e+06]]]\n",
      "X_grouped.transpose= <built-in method transpose of numpy.ndarray object at 0x000001EA92FCD870>\n",
      "X_grouped.transpose(0,2,1)= [[[2.7300000e+02]\n",
      "  [2.7300000e+02]\n",
      "  [1.0010000e+03]\n",
      "  [1.0000000e+01]\n",
      "  [1.0000000e+01]\n",
      "  [1.6604157e+18]\n",
      "  [2.0000000e+00]\n",
      "  [5.0000000e+06]\n",
      "  [5.0000000e+06]]]\n",
      "X_grouped= tf.Tensor(\n",
      "[[[2.7300000e+02]\n",
      "  [2.7300000e+02]\n",
      "  [1.0010000e+03]\n",
      "  [1.0000000e+01]\n",
      "  [1.0000000e+01]\n",
      "  [1.6604157e+18]\n",
      "  [2.0000000e+00]\n",
      "  [5.0000000e+06]\n",
      "  [5.0000000e+06]]], shape=(1, 9, 1), dtype=float32)\n",
      "y_fake= tf.Tensor([0], shape=(1,), dtype=int64)\n",
      "tensor_test= TensorDataset(X_grouped, y_fake)= <TensorDataset shapes: ((1, 9, 1), (1,)), types: (tf.float32, tf.int64)>\n",
      "test_dl= <BatchDataset shapes: ((None, 1, 9, 1), (None, 1)), types: (tf.float32, tf.int64)>\n",
      "Epoch 1/20\n",
      "1/1 [==============================] - 1s 1s/step - loss: 7832344474326843563121496293376.0000\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 12ms/step - loss: nan\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 12ms/step - loss: nan\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 12ms/step - loss: nan\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 13ms/step - loss: nan\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 12ms/step - loss: nan\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: nan\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 0s 8ms/step - loss: nan\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: nan\n",
      "///enter for loop in predict unseen data////\n",
      "batch= tf.Tensor(\n",
      "[[[[2.7300000e+02]\n",
      "   [2.7300000e+02]\n",
      "   [1.0010000e+03]\n",
      "   [1.0000000e+01]\n",
      "   [1.0000000e+01]\n",
      "   [1.6604157e+18]\n",
      "   [2.0000000e+00]\n",
      "   [5.0000000e+06]\n",
      "   [5.0000000e+06]]]], shape=(1, 1, 9, 1), dtype=float32)\n",
      "batch = batch[0]= tf.Tensor(\n",
      "[[[2.7300000e+02]\n",
      "  [2.7300000e+02]\n",
      "  [1.0010000e+03]\n",
      "  [1.0000000e+01]\n",
      "  [1.0000000e+01]\n",
      "  [1.6604157e+18]\n",
      "  [2.0000000e+00]\n",
      "  [5.0000000e+06]\n",
      "  [5.0000000e+06]]], shape=(1, 9, 1), dtype=float32)\n",
      "out=model(batch)= [[nan nan]]\n",
      "y_hat = tf.nn.log_softmax(out)= tf.Tensor([[nan nan]], shape=(1, 2), dtype=float32)\n",
      "y_hat = tf.argmax(y_hat)= tf.Tensor([0 0], shape=(2,), dtype=int64)\n",
      "y_hat = tf.get_static_value(y_hat, partial=False)= [0 0]\n",
      "ret += y_hat.tolist()= [0, 0]\n",
      "ret after for loop= [0, 0]\n",
      "ret[0] 0\n",
      "ret[0] == 0\n",
      "ret=predict_unseen_data(ai_model, celldata)= Normal\n",
      "celldata: [273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]\n",
      "Classification Normal\n",
      "////prediction doneee///////\n",
      "///////////////enter def run_prediction///////////////\n",
      "sample=[3735,...]= [3735, 0, 27648, 2295, 18, -1, 16383, -1, -1, -1]\n",
      "//////////enter if cell_data in run_prediction/////\n",
      "cell_data= [[273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]]\n",
      "pos=(pos + 1) % len(cell_data)= 0\n",
      "sample = cell_data[pos]= [273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]\n",
      "/////////////enter def predict/////////////////\n",
      "/////////////enter def load_model_parameters////////////////\n",
      "PATH = model.pth= model.pth\n",
      "cwd=os.getcwd= C:\\Users\\Mohammadreza\\Desktop\\My Class\\Proj-DC\\My Works\\Scheduling\\xApp\\mr7-main\\mr\n",
      "os.listdir(cwd)= ['.ipynb_checkpoints', 'assets', 'cells.csv', 'db.py', 'insert.py', 'keras_metadata.pb', 'main-changed-v3.ipynb', 'main-copy-v1.ipynb', 'main-Copy-v2.ipynb', 'main.py', 'model.pth', 'model.zip', 'mr_model', 'populate.py', 'saved_model.pb', 'sdl.py', 'ue.json.gz', 'valid.csv', 'variables', '__pycache__']\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_24 (LSTM)               (None, 9, 9)              396       \n",
      "_________________________________________________________________\n",
      "lstm_25 (LSTM)               (None, 9, 16)             1664      \n",
      "_________________________________________________________________\n",
      "lstm_26 (LSTM)               (None, 32)                6272      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 9,454\n",
      "Trainable params: 9,454\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_24 (LSTM)               (None, 9, 9)              396       \n",
      "_________________________________________________________________\n",
      "lstm_25 (LSTM)               (None, 9, 16)             1664      \n",
      "_________________________________________________________________\n",
      "lstm_26 (LSTM)               (None, 32)                6272      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 9,454\n",
      "Trainable params: 9,454\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "model=LSTClassifier= None\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\Mohammadreza\\Desktop\\My Class\\Proj-DC\\My Works\\Scheduling\\xApp\\mr7-main\\mr\\assets\n",
      "ai_model.summary()= <keras.engine.sequential.Sequential object at 0x000001EA90A75400>\n",
      "celldata: [273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]\n",
      "/////////////////enter def predict_unseen_data///////////////////\n",
      "unseen_data= [273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]\n",
      "np_data= [2.7300000e+02 2.7300000e+02 1.0010000e+03 1.0000000e+01 1.0000000e+01\n",
      " 1.6604157e+18 2.0000000e+00 5.0000000e+06 5.0000000e+06]\n",
      "newaxis= None\n",
      "X_grouped = np_data[newaxis, newaxis, :]= [[[2.7300000e+02 2.7300000e+02 1.0010000e+03 1.0000000e+01 1.0000000e+01\n",
      "   1.6604157e+18 2.0000000e+00 5.0000000e+06 5.0000000e+06]]]\n",
      "X_grouped.transpose(0, 2, 1)= [[[2.7300000e+02]\n",
      "  [2.7300000e+02]\n",
      "  [1.0010000e+03]\n",
      "  [1.0000000e+01]\n",
      "  [1.0000000e+01]\n",
      "  [1.6604157e+18]\n",
      "  [2.0000000e+00]\n",
      "  [5.0000000e+06]\n",
      "  [5.0000000e+06]]]\n",
      "X_grouped.transpose= <built-in method transpose of numpy.ndarray object at 0x000001EA8810A330>\n",
      "X_grouped.transpose(0,2,1)= [[[2.7300000e+02]\n",
      "  [2.7300000e+02]\n",
      "  [1.0010000e+03]\n",
      "  [1.0000000e+01]\n",
      "  [1.0000000e+01]\n",
      "  [1.6604157e+18]\n",
      "  [2.0000000e+00]\n",
      "  [5.0000000e+06]\n",
      "  [5.0000000e+06]]]\n",
      "X_grouped= tf.Tensor(\n",
      "[[[2.7300000e+02]\n",
      "  [2.7300000e+02]\n",
      "  [1.0010000e+03]\n",
      "  [1.0000000e+01]\n",
      "  [1.0000000e+01]\n",
      "  [1.6604157e+18]\n",
      "  [2.0000000e+00]\n",
      "  [5.0000000e+06]\n",
      "  [5.0000000e+06]]], shape=(1, 9, 1), dtype=float32)\n",
      "y_fake= tf.Tensor([0], shape=(1,), dtype=int64)\n",
      "tensor_test= TensorDataset(X_grouped, y_fake)= <TensorDataset shapes: ((1, 9, 1), (1,)), types: (tf.float32, tf.int64)>\n",
      "test_dl= <BatchDataset shapes: ((None, 1, 9, 1), (None, 1)), types: (tf.float32, tf.int64)>\n",
      "Epoch 1/20\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5357107529785008160668641434533888.0000\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 12ms/step - loss: nan\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 12ms/step - loss: nan\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 0s 9ms/step - loss: nan\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 8ms/step - loss: nan\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 0s 8ms/step - loss: nan\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 0s 8ms/step - loss: nan\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 0s 8ms/step - loss: nan\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 0s 9ms/step - loss: nan\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 0s 9ms/step - loss: nan\n",
      "///enter for loop in predict unseen data////\n",
      "batch= tf.Tensor(\n",
      "[[[[2.7300000e+02]\n",
      "   [2.7300000e+02]\n",
      "   [1.0010000e+03]\n",
      "   [1.0000000e+01]\n",
      "   [1.0000000e+01]\n",
      "   [1.6604157e+18]\n",
      "   [2.0000000e+00]\n",
      "   [5.0000000e+06]\n",
      "   [5.0000000e+06]]]], shape=(1, 1, 9, 1), dtype=float32)\n",
      "batch = batch[0]= tf.Tensor(\n",
      "[[[2.7300000e+02]\n",
      "  [2.7300000e+02]\n",
      "  [1.0010000e+03]\n",
      "  [1.0000000e+01]\n",
      "  [1.0000000e+01]\n",
      "  [1.6604157e+18]\n",
      "  [2.0000000e+00]\n",
      "  [5.0000000e+06]\n",
      "  [5.0000000e+06]]], shape=(1, 9, 1), dtype=float32)\n",
      "out=model(batch)= [[nan nan]]\n",
      "y_hat = tf.nn.log_softmax(out)= tf.Tensor([[nan nan]], shape=(1, 2), dtype=float32)\n",
      "y_hat = tf.argmax(y_hat)= tf.Tensor([0 0], shape=(2,), dtype=int64)\n",
      "y_hat = tf.get_static_value(y_hat, partial=False)= [0 0]\n",
      "ret += y_hat.tolist()= [0, 0]\n",
      "ret after for loop= [0, 0]\n",
      "ret[0] 0\n",
      "ret[0] == 0\n",
      "ret=predict_unseen_data(ai_model, celldata)= Normal\n",
      "celldata: [273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]\n",
      "Classification Normal\n",
      "////prediction doneee///////\n",
      "///////////////enter def run_prediction///////////////\n",
      "sample=[3735,...]= [3735, 0, 27648, 2295, 18, -1, 16383, -1, -1, -1]\n",
      "//////////enter if cell_data in run_prediction/////\n",
      "cell_data= [[273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]]\n",
      "pos=(pos + 1) % len(cell_data)= 0\n",
      "sample = cell_data[pos]= [273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]\n",
      "/////////////enter def predict/////////////////\n",
      "/////////////enter def load_model_parameters////////////////\n",
      "PATH = model.pth= model.pth\n",
      "cwd=os.getcwd= C:\\Users\\Mohammadreza\\Desktop\\My Class\\Proj-DC\\My Works\\Scheduling\\xApp\\mr7-main\\mr\n",
      "os.listdir(cwd)= ['.ipynb_checkpoints', 'assets', 'cells.csv', 'db.py', 'insert.py', 'keras_metadata.pb', 'main-changed-v3.ipynb', 'main-copy-v1.ipynb', 'main-Copy-v2.ipynb', 'main.py', 'model.pth', 'model.zip', 'mr_model', 'populate.py', 'saved_model.pb', 'sdl.py', 'ue.json.gz', 'valid.csv', 'variables', '__pycache__']\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_27 (LSTM)               (None, 9, 9)              396       \n",
      "_________________________________________________________________\n",
      "lstm_28 (LSTM)               (None, 9, 16)             1664      \n",
      "_________________________________________________________________\n",
      "lstm_29 (LSTM)               (None, 32)                6272      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 9,454\n",
      "Trainable params: 9,454\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_27 (LSTM)               (None, 9, 9)              396       \n",
      "_________________________________________________________________\n",
      "lstm_28 (LSTM)               (None, 9, 16)             1664      \n",
      "_________________________________________________________________\n",
      "lstm_29 (LSTM)               (None, 32)                6272      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 9,454\n",
      "Trainable params: 9,454\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "model=LSTClassifier= None\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\Mohammadreza\\Desktop\\My Class\\Proj-DC\\My Works\\Scheduling\\xApp\\mr7-main\\mr\\assets\n",
      "ai_model.summary()= <keras.engine.sequential.Sequential object at 0x000001EA89305550>\n",
      "celldata: [273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]\n",
      "/////////////////enter def predict_unseen_data///////////////////\n",
      "unseen_data= [273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]\n",
      "np_data= [2.7300000e+02 2.7300000e+02 1.0010000e+03 1.0000000e+01 1.0000000e+01\n",
      " 1.6604157e+18 2.0000000e+00 5.0000000e+06 5.0000000e+06]\n",
      "newaxis= None\n",
      "X_grouped = np_data[newaxis, newaxis, :]= [[[2.7300000e+02 2.7300000e+02 1.0010000e+03 1.0000000e+01 1.0000000e+01\n",
      "   1.6604157e+18 2.0000000e+00 5.0000000e+06 5.0000000e+06]]]\n",
      "X_grouped.transpose(0, 2, 1)= [[[2.7300000e+02]\n",
      "  [2.7300000e+02]\n",
      "  [1.0010000e+03]\n",
      "  [1.0000000e+01]\n",
      "  [1.0000000e+01]\n",
      "  [1.6604157e+18]\n",
      "  [2.0000000e+00]\n",
      "  [5.0000000e+06]\n",
      "  [5.0000000e+06]]]\n",
      "X_grouped.transpose= <built-in method transpose of numpy.ndarray object at 0x000001EA8927EBD0>\n",
      "X_grouped.transpose(0,2,1)= [[[2.7300000e+02]\n",
      "  [2.7300000e+02]\n",
      "  [1.0010000e+03]\n",
      "  [1.0000000e+01]\n",
      "  [1.0000000e+01]\n",
      "  [1.6604157e+18]\n",
      "  [2.0000000e+00]\n",
      "  [5.0000000e+06]\n",
      "  [5.0000000e+06]]]\n",
      "X_grouped= tf.Tensor(\n",
      "[[[2.7300000e+02]\n",
      "  [2.7300000e+02]\n",
      "  [1.0010000e+03]\n",
      "  [1.0000000e+01]\n",
      "  [1.0000000e+01]\n",
      "  [1.6604157e+18]\n",
      "  [2.0000000e+00]\n",
      "  [5.0000000e+06]\n",
      "  [5.0000000e+06]]], shape=(1, 9, 1), dtype=float32)\n",
      "y_fake= tf.Tensor([0], shape=(1,), dtype=int64)\n",
      "tensor_test= TensorDataset(X_grouped, y_fake)= <TensorDataset shapes: ((1, 9, 1), (1,)), types: (tf.float32, tf.int64)>\n",
      "test_dl= <BatchDataset shapes: ((None, 1, 9, 1), (None, 1)), types: (tf.float32, tf.int64)>\n",
      "Epoch 1/20\n",
      "1/1 [==============================] - 1s 1s/step - loss: 618679877445982626447632629760.0000\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 12ms/step - loss: nan\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 12ms/step - loss: nan\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 12ms/step - loss: nan\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 12ms/step - loss: nan\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 12ms/step - loss: nan\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 0s 8ms/step - loss: nan\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 0s 9ms/step - loss: nan\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 8ms/step - loss: nan\n",
      "///enter for loop in predict unseen data////\n",
      "batch= tf.Tensor(\n",
      "[[[[2.7300000e+02]\n",
      "   [2.7300000e+02]\n",
      "   [1.0010000e+03]\n",
      "   [1.0000000e+01]\n",
      "   [1.0000000e+01]\n",
      "   [1.6604157e+18]\n",
      "   [2.0000000e+00]\n",
      "   [5.0000000e+06]\n",
      "   [5.0000000e+06]]]], shape=(1, 1, 9, 1), dtype=float32)\n",
      "batch = batch[0]= tf.Tensor(\n",
      "[[[2.7300000e+02]\n",
      "  [2.7300000e+02]\n",
      "  [1.0010000e+03]\n",
      "  [1.0000000e+01]\n",
      "  [1.0000000e+01]\n",
      "  [1.6604157e+18]\n",
      "  [2.0000000e+00]\n",
      "  [5.0000000e+06]\n",
      "  [5.0000000e+06]]], shape=(1, 9, 1), dtype=float32)\n",
      "out=model(batch)= [[nan nan]]\n",
      "y_hat = tf.nn.log_softmax(out)= tf.Tensor([[nan nan]], shape=(1, 2), dtype=float32)\n",
      "y_hat = tf.argmax(y_hat)= tf.Tensor([0 0], shape=(2,), dtype=int64)\n",
      "y_hat = tf.get_static_value(y_hat, partial=False)= [0 0]\n",
      "ret += y_hat.tolist()= [0, 0]\n",
      "ret after for loop= [0, 0]\n",
      "ret[0] 0\n",
      "ret[0] == 0\n",
      "ret=predict_unseen_data(ai_model, celldata)= Normal\n",
      "celldata: [273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]\n",
      "Classification Normal\n",
      "////prediction doneee///////\n",
      "///////////////enter def run_prediction///////////////\n",
      "sample=[3735,...]= [3735, 0, 27648, 2295, 18, -1, 16383, -1, -1, -1]\n",
      "//////////enter if cell_data in run_prediction/////\n",
      "cell_data= [[273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]]\n",
      "pos=(pos + 1) % len(cell_data)= 0\n",
      "sample = cell_data[pos]= [273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]\n",
      "/////////////enter def predict/////////////////\n",
      "/////////////enter def load_model_parameters////////////////\n",
      "PATH = model.pth= model.pth\n",
      "cwd=os.getcwd= C:\\Users\\Mohammadreza\\Desktop\\My Class\\Proj-DC\\My Works\\Scheduling\\xApp\\mr7-main\\mr\n",
      "os.listdir(cwd)= ['.ipynb_checkpoints', 'assets', 'cells.csv', 'db.py', 'insert.py', 'keras_metadata.pb', 'main-changed-v3.ipynb', 'main-copy-v1.ipynb', 'main-Copy-v2.ipynb', 'main.py', 'model.pth', 'model.zip', 'mr_model', 'populate.py', 'saved_model.pb', 'sdl.py', 'ue.json.gz', 'valid.csv', 'variables', '__pycache__']\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_30 (LSTM)               (None, 9, 9)              396       \n",
      "_________________________________________________________________\n",
      "lstm_31 (LSTM)               (None, 9, 16)             1664      \n",
      "_________________________________________________________________\n",
      "lstm_32 (LSTM)               (None, 32)                6272      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 9,454\n",
      "Trainable params: 9,454\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_30 (LSTM)               (None, 9, 9)              396       \n",
      "_________________________________________________________________\n",
      "lstm_31 (LSTM)               (None, 9, 16)             1664      \n",
      "_________________________________________________________________\n",
      "lstm_32 (LSTM)               (None, 32)                6272      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 9,454\n",
      "Trainable params: 9,454\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "model=LSTClassifier= None\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\Mohammadreza\\Desktop\\My Class\\Proj-DC\\My Works\\Scheduling\\xApp\\mr7-main\\mr\\assets\n",
      "ai_model.summary()= <keras.engine.sequential.Sequential object at 0x000001EA90CE7280>\n",
      "celldata: [273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]\n",
      "/////////////////enter def predict_unseen_data///////////////////\n",
      "unseen_data= [273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]\n",
      "np_data= [2.7300000e+02 2.7300000e+02 1.0010000e+03 1.0000000e+01 1.0000000e+01\n",
      " 1.6604157e+18 2.0000000e+00 5.0000000e+06 5.0000000e+06]\n",
      "newaxis= None\n",
      "X_grouped = np_data[newaxis, newaxis, :]= [[[2.7300000e+02 2.7300000e+02 1.0010000e+03 1.0000000e+01 1.0000000e+01\n",
      "   1.6604157e+18 2.0000000e+00 5.0000000e+06 5.0000000e+06]]]\n",
      "X_grouped.transpose(0, 2, 1)= [[[2.7300000e+02]\n",
      "  [2.7300000e+02]\n",
      "  [1.0010000e+03]\n",
      "  [1.0000000e+01]\n",
      "  [1.0000000e+01]\n",
      "  [1.6604157e+18]\n",
      "  [2.0000000e+00]\n",
      "  [5.0000000e+06]\n",
      "  [5.0000000e+06]]]\n",
      "X_grouped.transpose= <built-in method transpose of numpy.ndarray object at 0x000001EA8B51C990>\n",
      "X_grouped.transpose(0,2,1)= [[[2.7300000e+02]\n",
      "  [2.7300000e+02]\n",
      "  [1.0010000e+03]\n",
      "  [1.0000000e+01]\n",
      "  [1.0000000e+01]\n",
      "  [1.6604157e+18]\n",
      "  [2.0000000e+00]\n",
      "  [5.0000000e+06]\n",
      "  [5.0000000e+06]]]\n",
      "X_grouped= tf.Tensor(\n",
      "[[[2.7300000e+02]\n",
      "  [2.7300000e+02]\n",
      "  [1.0010000e+03]\n",
      "  [1.0000000e+01]\n",
      "  [1.0000000e+01]\n",
      "  [1.6604157e+18]\n",
      "  [2.0000000e+00]\n",
      "  [5.0000000e+06]\n",
      "  [5.0000000e+06]]], shape=(1, 9, 1), dtype=float32)\n",
      "y_fake= tf.Tensor([0], shape=(1,), dtype=int64)\n",
      "tensor_test= TensorDataset(X_grouped, y_fake)= <TensorDataset shapes: ((1, 9, 1), (1,)), types: (tf.float32, tf.int64)>\n",
      "test_dl= <BatchDataset shapes: ((None, 1, 9, 1), (None, 1)), types: (tf.float32, tf.int64)>\n",
      "Epoch 1/20\n",
      "1/1 [==============================] - 1s 1s/step - loss: 235276728338607409811423232.0000\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 12ms/step - loss: nan\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 0s 12ms/step - loss: nan\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan\n",
      "///enter for loop in predict unseen data////\n",
      "batch= tf.Tensor(\n",
      "[[[[2.7300000e+02]\n",
      "   [2.7300000e+02]\n",
      "   [1.0010000e+03]\n",
      "   [1.0000000e+01]\n",
      "   [1.0000000e+01]\n",
      "   [1.6604157e+18]\n",
      "   [2.0000000e+00]\n",
      "   [5.0000000e+06]\n",
      "   [5.0000000e+06]]]], shape=(1, 1, 9, 1), dtype=float32)\n",
      "batch = batch[0]= tf.Tensor(\n",
      "[[[2.7300000e+02]\n",
      "  [2.7300000e+02]\n",
      "  [1.0010000e+03]\n",
      "  [1.0000000e+01]\n",
      "  [1.0000000e+01]\n",
      "  [1.6604157e+18]\n",
      "  [2.0000000e+00]\n",
      "  [5.0000000e+06]\n",
      "  [5.0000000e+06]]], shape=(1, 9, 1), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out=model(batch)= [[nan nan]]\n",
      "y_hat = tf.nn.log_softmax(out)= tf.Tensor([[nan nan]], shape=(1, 2), dtype=float32)\n",
      "y_hat = tf.argmax(y_hat)= tf.Tensor([0 0], shape=(2,), dtype=int64)\n",
      "y_hat = tf.get_static_value(y_hat, partial=False)= [0 0]\n",
      "ret += y_hat.tolist()= [0, 0]\n",
      "ret after for loop= [0, 0]\n",
      "ret[0] 0\n",
      "ret[0] == 0\n",
      "ret=predict_unseen_data(ai_model, celldata)= Normal\n",
      "celldata: [273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]\n",
      "Classification Normal\n",
      "////prediction doneee///////\n",
      "///////////////enter def run_prediction///////////////\n",
      "sample=[3735,...]= [3735, 0, 27648, 2295, 18, -1, 16383, -1, -1, -1]\n",
      "//////////enter if cell_data in run_prediction/////\n",
      "cell_data= [[273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]]\n",
      "pos=(pos + 1) % len(cell_data)= 0\n",
      "sample = cell_data[pos]= [273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]\n",
      "/////////////enter def predict/////////////////\n",
      "/////////////enter def load_model_parameters////////////////\n",
      "PATH = model.pth= model.pth\n",
      "cwd=os.getcwd= C:\\Users\\Mohammadreza\\Desktop\\My Class\\Proj-DC\\My Works\\Scheduling\\xApp\\mr7-main\\mr\n",
      "os.listdir(cwd)= ['.ipynb_checkpoints', 'assets', 'cells.csv', 'db.py', 'insert.py', 'keras_metadata.pb', 'main-changed-v3.ipynb', 'main-copy-v1.ipynb', 'main-Copy-v2.ipynb', 'main.py', 'model.pth', 'model.zip', 'mr_model', 'populate.py', 'saved_model.pb', 'sdl.py', 'ue.json.gz', 'valid.csv', 'variables', '__pycache__']\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_33 (LSTM)               (None, 9, 9)              396       \n",
      "_________________________________________________________________\n",
      "lstm_34 (LSTM)               (None, 9, 16)             1664      \n",
      "_________________________________________________________________\n",
      "lstm_35 (LSTM)               (None, 32)                6272      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 9,454\n",
      "Trainable params: 9,454\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_33 (LSTM)               (None, 9, 9)              396       \n",
      "_________________________________________________________________\n",
      "lstm_34 (LSTM)               (None, 9, 16)             1664      \n",
      "_________________________________________________________________\n",
      "lstm_35 (LSTM)               (None, 32)                6272      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 9,454\n",
      "Trainable params: 9,454\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "model=LSTClassifier= None\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\Mohammadreza\\Desktop\\My Class\\Proj-DC\\My Works\\Scheduling\\xApp\\mr7-main\\mr\\assets\n",
      "ai_model.summary()= <keras.engine.sequential.Sequential object at 0x000001EA9843F730>\n",
      "celldata: [273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]\n",
      "/////////////////enter def predict_unseen_data///////////////////\n",
      "unseen_data= [273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]\n",
      "np_data= [2.7300000e+02 2.7300000e+02 1.0010000e+03 1.0000000e+01 1.0000000e+01\n",
      " 1.6604157e+18 2.0000000e+00 5.0000000e+06 5.0000000e+06]\n",
      "newaxis= None\n",
      "X_grouped = np_data[newaxis, newaxis, :]= [[[2.7300000e+02 2.7300000e+02 1.0010000e+03 1.0000000e+01 1.0000000e+01\n",
      "   1.6604157e+18 2.0000000e+00 5.0000000e+06 5.0000000e+06]]]\n",
      "X_grouped.transpose(0, 2, 1)= [[[2.7300000e+02]\n",
      "  [2.7300000e+02]\n",
      "  [1.0010000e+03]\n",
      "  [1.0000000e+01]\n",
      "  [1.0000000e+01]\n",
      "  [1.6604157e+18]\n",
      "  [2.0000000e+00]\n",
      "  [5.0000000e+06]\n",
      "  [5.0000000e+06]]]\n",
      "X_grouped.transpose= <built-in method transpose of numpy.ndarray object at 0x000001EA8E9B0AB0>\n",
      "X_grouped.transpose(0,2,1)= [[[2.7300000e+02]\n",
      "  [2.7300000e+02]\n",
      "  [1.0010000e+03]\n",
      "  [1.0000000e+01]\n",
      "  [1.0000000e+01]\n",
      "  [1.6604157e+18]\n",
      "  [2.0000000e+00]\n",
      "  [5.0000000e+06]\n",
      "  [5.0000000e+06]]]\n",
      "X_grouped= tf.Tensor(\n",
      "[[[2.7300000e+02]\n",
      "  [2.7300000e+02]\n",
      "  [1.0010000e+03]\n",
      "  [1.0000000e+01]\n",
      "  [1.0000000e+01]\n",
      "  [1.6604157e+18]\n",
      "  [2.0000000e+00]\n",
      "  [5.0000000e+06]\n",
      "  [5.0000000e+06]]], shape=(1, 9, 1), dtype=float32)\n",
      "y_fake= tf.Tensor([0], shape=(1,), dtype=int64)\n",
      "tensor_test= TensorDataset(X_grouped, y_fake)= <TensorDataset shapes: ((1, 9, 1), (1,)), types: (tf.float32, tf.int64)>\n",
      "test_dl= <BatchDataset shapes: ((None, 1, 9, 1), (None, 1)), types: (tf.float32, tf.int64)>\n",
      "Epoch 1/20\n",
      "1/1 [==============================] - 2s 2s/step - loss: 24714904689197398312648903229440.0000\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 12ms/step - loss: nan\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 12ms/step - loss: nan\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 9ms/step - loss: nan\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 12ms/step - loss: nan\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 0s 12ms/step - loss: nan\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: nan\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 0s 8ms/step - loss: nan\n",
      "///enter for loop in predict unseen data////\n",
      "batch= tf.Tensor(\n",
      "[[[[2.7300000e+02]\n",
      "   [2.7300000e+02]\n",
      "   [1.0010000e+03]\n",
      "   [1.0000000e+01]\n",
      "   [1.0000000e+01]\n",
      "   [1.6604157e+18]\n",
      "   [2.0000000e+00]\n",
      "   [5.0000000e+06]\n",
      "   [5.0000000e+06]]]], shape=(1, 1, 9, 1), dtype=float32)\n",
      "batch = batch[0]= tf.Tensor(\n",
      "[[[2.7300000e+02]\n",
      "  [2.7300000e+02]\n",
      "  [1.0010000e+03]\n",
      "  [1.0000000e+01]\n",
      "  [1.0000000e+01]\n",
      "  [1.6604157e+18]\n",
      "  [2.0000000e+00]\n",
      "  [5.0000000e+06]\n",
      "  [5.0000000e+06]]], shape=(1, 9, 1), dtype=float32)\n",
      "out=model(batch)= [[nan nan]]\n",
      "y_hat = tf.nn.log_softmax(out)= tf.Tensor([[nan nan]], shape=(1, 2), dtype=float32)\n",
      "y_hat = tf.argmax(y_hat)= tf.Tensor([0 0], shape=(2,), dtype=int64)\n",
      "y_hat = tf.get_static_value(y_hat, partial=False)= [0 0]\n",
      "ret += y_hat.tolist()= [0, 0]\n",
      "ret after for loop= [0, 0]\n",
      "ret[0] 0\n",
      "ret[0] == 0\n",
      "ret=predict_unseen_data(ai_model, celldata)= Normal\n",
      "celldata: [273, 273, 1001, 10, 10, 1660415639684690000, 2, 5000000, 5000000]\n",
      "Classification Normal\n",
      "////prediction doneee///////\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-41254fe0d1e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m#xapp= entry(self)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m#xapp.run()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mentry\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-a84414e50f38>\u001b[0m in \u001b[0;36mentry\u001b[1;34m()\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;31m#print('////while True in entry/////')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m         \u001b[0mschedule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_pending\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mrun_prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\schedule\\__init__.py\u001b[0m in \u001b[0;36mrun_pending\u001b[1;34m()\u001b[0m\n\u001b[0;32m    778\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mdefault\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0minstance\u001b[0m \u001b[1;33m<\u001b[0m\u001b[0mdefault_scheduler\u001b[0m\u001b[1;33m>\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m     \"\"\"\n\u001b[1;32m--> 780\u001b[1;33m     \u001b[0mdefault_scheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_pending\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\schedule\\__init__.py\u001b[0m in \u001b[0;36mrun_pending\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     97\u001b[0m         \"\"\"\n\u001b[0;32m     98\u001b[0m         \u001b[0mrunnable_jobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mjob\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjobs\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_run\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mjob\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrunnable_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_job\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('////////////////entered Starrrrrrrrrrrt///////////////////')\n",
    "\"\"\"\n",
    "This is a convenience function that allows this xapp to run in Docker\n",
    "for \"real\" (no thread, real SDL), but also easily modified for unit testing\n",
    "(e.g., use_fake_sdl). The defaults for this function are for the Dockerized xapp.\n",
    "\"\"\"\n",
    "thread = False\n",
    "global xapp, ai_model\n",
    "#fake_sdl = getenv(\"USE_FAKE_SDL\", None)\n",
    "#xapp = Xapp(entrypoint=entry, rmr_port=4560, use_fake_sdl=False)\n",
    "\n",
    "connectdb(thread)\n",
    "print('///////come back from connectdb////////')\n",
    "ai_model = load_model_parameter()\n",
    "print('ai_model.summary in start=', ai_model)\n",
    "\n",
    "use_fake_sdl=False\n",
    "rmr_port=4560\n",
    "#xapp= entry(self)\n",
    "#xapp.run()\n",
    "entry()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
